{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(76, 111, 175); padding: 10px; border-radius: 10px;\">\n",
    "    <h1 style=\"color: Black ; text-align: center; font-weight: bold; font-family: Comic Sans MS, cursive, sans-serif;\">\n",
    "    Arabic Text Summarization\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model , dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model =\"UBC-NLP/AraT5v2-base-1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'summary'],\n",
       "        num_rows: 141467\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"FahdSeddik/AGS-Corpus\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (141467, 3)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b44ca597-d290-4e8d-9ef2-d184a9884cfc",
       "rows": [
        [
         "46622",
         "خلال العقد الأول من العقد الأول من القرن الحادي والعشرين، كانت هناك مخاوف جدية من أن الكويكب الذي يبلغ عرضه 325 مترًا (1066 قدمًا) شكّل أبوفيس خطرًا بالتأثير على الأرض في عام 2036، وقد أدت البيانات الأولية غير المكتملة من قبل علماء الفلك باستخدام مسوحات أرضية للسماء إلى حساب مخاطر المستوى 4 على مخطط مخاطر تأثير مقياس تورينو، في يوليو 2005 طلبت (B612) رسميًا من ناسا التحقيق في احتمال أن يكون مدار الكويكب بعد عام 2029 في صدى مداري مع الأرض، مما سيزيد من احتمالية حدوث اصطدام في المستقبل. طلبت المؤسسة أيضًا من وكالة ناسا التحقيق فيما إذا كان يجب وضع جهاز إرسال واستقبال على الكويكب لتمكين تتبع أكثر دقة لكيفية تغيير مداره بواسطة تأثير ياركوفسكي.بحلول عام 2008، قدمت (B612) تقديرات حول ممر بعرض 30 كيلومترًا، يسمى «مسار الخطر»، والذي من شأنه أن يمتد عبر سطح الأرض إذا حدث تأثير، كجزء من جهوده لتطوير استراتيجيات انحراف قابلة للتطبيق. امتد مسار المخاطر المحسوب من كازاخستان عبر جنوب روسيا عبر سيبيريا، عبر المحيط الهادئ، ثم يمينًا بين نيكاراغوا وكوستاريكا، مروراً بشمال كولومبيا وفنزويلا، وانتهى في المحيط الأطلسي قبل الوصول إلى إفريقيا،  في ذلك الوقت قدرت محاكاة حاسوبية أن التأثير الافتراضي لأبوفيس في دول مثل كولومبيا وفنزويلا، يمكن أن يؤدي إلى أكثر من 10 ملايين ضحية، بالتناوب يمكن أن يؤدي التأثير في المحيط الأطلسي أو المحيط الهادئ إلى حدوث تسونامي مميت يزيد ارتفاعه عن 240 مترًا (حوالي 800 قدم)، وهو قادر على تدمير العديد من المناطق والمدن الساحلية،  قامت سلسلة من الملاحظات اللاحقة الأكثر دقة لـ 99942 أبوفيس، جنبًا إلى جنب مع استعادة البيانات غير المرئية سابقًا، بمراجعة احتمالات حدوث تصادم في عام 2036 على أنها لا شيء تقريبًا، واستبعدت ذلك فعليًا.",
         "في العقد الأول من القرن الحادي والعشرين، كان هناك قلق كبير بشأن الكويكب الذي يبلغ عرضه 325 مترًا، أبوفيس، وتأثيره على الأرض في عام 2036. في يوليو 2005، طلبت (B612) التحقيق في تهديد المستوى 4 للكويكب، واقترحت وضع جهاز إرسال واستقبال على سطحه لتتبع تغير مداره. تم تقدير ممر الخطر للكويكب في عام 2008 بعرض 30 كيلومترًا. تشير المحاكاة الكمبيوترية إلى أن تأثير أبوفيس يمكن أن يتسبب في تسونامي مدمر وخسائر بشرية كبيرة. ومع ذلك، تشير المراجعات اللاحقة إلى أن احتمالية التصادم قليلة جدًا وتم استبعادها."
        ],
        [
         "9159",
         "بعض أنصار صباحي يعارضون ترشحه في حالة ترشح السيسي وقال صباحي لمؤيديه في القاهرة إنه سيترشح لأن الثورة يجب ان تحكم وفي كلمة أمام حشد من أنصاره أضاف صباحيالمواطن حمدين صباحي قراري الشخصي أن اخوض معركة الانتخابات الرئاسية المقبلة وقال إن معركته هي معركة الثورة وجاء صباحي ثالثا في انتخابات 2012 التي فاز بها الدكتور محمد مرسي المنتمي لجماعة الاخوان المسلمين والذي عزله الجيش في يوليو تموز الماضي بعد احتجاجات حاشدة على حكمه مواضيع قد تهمك نهاية تفتيت ثورة 30 يونيو وأغلب مؤيدي صباحي من الليبراليين والجماعات اليسارية الشبابية التي ترفض حكم الجيش أوالاسلاميين وقال تكتل التيار الشعبي الذي أسسه صباحي إنه سيتخذ في مؤتمره يوم الأربعاء المقبل قرارا نهائية بشأن مسألة دعم ترشيح صباحي وقال محمود بدر مؤسس حركة تمرد إن عضوي تمرد اللذين حضر إعلان صباحي ترشحه للرئاسة لا يمثلان إلا نفسيهما وأضاف بدر إن تمرد أعلنت بالفعل دعمها للسيسي وكانت الحركة قد لعبت دورا نشطا في الحملة الشعبية التي أدت إلى مظاهرات الثلاثين من يونيوحزيران الماضي ضد مرسي وأعلن خالد يوسف المخرج السينمائي المعروف والمقرب من صباحي معارضته لترشح صباحي للرئاسة  وحذر من أن هذا الترشح يشتت شمل ثورة 30 يونيو وكان صباحي قد أرجأ اعلان ترشحه أسابيع قائلا إن قراره يتوقف على ترشح السيسي لا حرية ولا ديمقراطية ولم يعلن رسميا في مصر بعد موعد لفتح باب الترشح لانتخابات الرئاسة لكن المرجح ان يتم الاعلان عن الموعد خلال ايام قليلة لاسيما بعد اعلان الرئيس المصري المؤقت عدلي منصور اعادة ترتيب الاستحقاقات الانتخابية في خارطة المستقبل لتكون الانتخابية الرئاسية قبل البرلمانية وينص الدستور على ان ينتهي انجاز هذا الاستحقاق في موعد اقصاه ثلاثة شهور من اعتماد الدستور رسميا ولم يعلن السيسي موقفه رسميا من الترشح رغم موافقة المجلس الاعلي للقوات المسلحة رسميا على قراره المحتمل بالترشح ويعارض بعض انصار صباحي ترشحه للرئاسة في حال ترشح وزير الدفاع ويحظى الوزير بشعبية بين كثير من المصريين الذين شعروا بالارتياح لانتهاء مرسي  ويرى هؤلاء في السيسي قائدا قويا تحتاجه مصر لإخراجها الأزمة الراهنة وتضفي وسائل الاعلام الرسمية والخاصة عليه هالة من القوة والبطولة وقال الدكتورعبد المنعم أبو الفتوح رئيس حزب مصر القوية والقيادي السابق في جماعة الإخوان المسملين إنه لا ينوي خوض انتخابات الرئاسة المرتقبة لأن الظروف الحالية ليست حرة ولا ديمقراطية وكان أبو الفتوح قد احتل المركز الرابع في انتخابات الرئاسة السابقة",
         "صباحي يؤيد ترشحه في الانتخابات الرئاسية المقبلة ويعتبر معركته معركة الثورة، وقد احتل المركز الثالث في الانتخابات السابقة. مؤيديه من الليبراليين والجماعات اليسارية الشبابية. تكتل التيار الشعبي سيعلن قراره بدعم ترشيحه. حركة تمرد أعلنت دعمها للسيسي. المخرج السينمائي خالد يوسف يعارض ترشحه. صباحي قد أرجأ اعلان ترشحه. موعد انتخابات الرئاسة سيتم الاعلان عنه قريباً. الدستور ينص علىانجاز هذا الاستحقاق في غضون ثلاثة أشهر من اعتماد الدستور. لم يعلن السيسي موقفه من الترشح. بعض انصار صباحي يعارضون ترشحه في حال ترشح وزير الدفاع. الوسائل الاعلامية تضفي عليه هالة من القوة. رئيس حزب مصر القوية يعتبر الظروف غير مناسبة للترشح."
        ],
        [
         "73902",
         "كان العميد محمد عتمان، مدير مباحث التموين بالغربية، قد تلقى إخطارا من ضباط الإدارة بنتائج الحملة، والتي أسفرت عن ضبط مخالفة متنوعة شملت مخالفات البيع بأزيد من التسعيرة وعدم الإعلان عن الأسعار وتجميع خبز مدعم وتجميع خبز بلدي مجفف إلى جانب ضبط سلع مجهولة المصدر ومغشوشة ومنتهية الصلاحية وتلاعب في الأوزان. وتم تحرير المحاضر اللازمة لجميع المخالفين، وجاري إخطار النيابات المختصة للمراكز والأقسام. ",
         "ضبطت الحملة في الغربية مخالفات تشمل: البيع بأزيد من التسعيرة، عدم الإعلان عن الأسعار، تجميع خبز مدعم وبلدي، سلع مجهولة المصدر ومغشوشة ومنتهية الصلاحية."
        ],
        [
         "26781",
         "رد فعل الجمهور تجاه حكايات بنات دفع منتج المسلسل طارق الجناينى لإنتاج جزء جديد من المسلسل، إلا أن الجمهور صدم من خبر غياب حورية فرغلى، وريهام أيمن التى ابتعدت عن التمثيل الفترة الماضية، وتمت الاستعانة بالفنانتين ندا موسى وإنجى المقدم، حيث ظن البعض أنهما ستحلان محل حورية وريهام، إلا أن أسرة العمل استطاعت أن تتحايل على غيابهما، أن تزرع فى وسط تلك الشلة صديقتين أخريين بخلاف كوكى حورية فرغلى و مريم مريم أمين. أحداث الحلقة الأولى من الجزء الثانى من المسلسل شهد تفسيرا لغياب حورية وريهام، وهو أن الأولى عادت لزوجها كريم، وسافرا سويا إلى أمريكا، أما الأخرى، فتلقى زوجها عرضًا للعمل في أوغندا، وفي نفس الوقت، تعرفت أحلام على خديجة إنجى المقدم، و ملك ندا موسى، وأصبحتا عضوتين مهمتين في الشلة، ومحركتين للأحداث. ",
         "رد فعل الجمهور تجاه حكايات بنات دفع منتج المسلسل طارق الجناينى لإنتاج جزء جديد، لكن الصدمة كانت غياب حورية فرغلى وريهام أيمن. تم استبدالهما بندا موسى وإنجى المقدم. تم توضيح غياب حورية وريهام في الحلقة الأولى. تعرف أحلام على خديجة إنجى المقدم وملك ندا موسى، وأصبحتا عضوتين هامتين في الشلة ودافعين للأحداث."
        ],
        [
         "40492",
         "هذا وقد اقيم الحفل في كل من الرياض والظهران والجبيل وتبوك وجدة وخميس مشيط، حيث تم تقديم العديد من الفقرات والفعاليات والمسابقات التي لقيت إعجاباً وشهدت تفاعلاً كبيراً من الحضور، كما تم تكريم الموظفين الذين أمضوا سنوات طويلة في خدمة الشركة وتم تقديم شهادات الشكر والتقدير والهدايا التذكارية المتنوعة، وبعد ذلك قام عدد من فرق الفنون الشعبية بتقديم عرض للفلكلور الشعبي التي تميز كل منطقة. وقال منذر بن محمود طيب مدير عام العلاقات العامة والتواصل بالشركة انها حرصت على أن تقيم هذه المناسبة لتضم جميع مكاتبها في المملكة لإيجاد فرصة أكبر للتعارف بين الموظفين وللتعبير عن امتنان الشركة للجهود التي بذلها جميع منسوبيها خلال العام وحققوا النجاح المأمول في إطار من التعاون بين كافة قطاعات ومكاتب الشركة في المملكة. ",
         "تم تنظيم حفل في الرياض والظهران والجبيل وتبوك وجدة وخميس مشيط. قدمت فقرات وفعاليات ومسابقات، وتم تكريم الموظفين الذين خدموا الشركة لسنوات طويلة. قامت فرق الفنون الشعبية بعرض فلكلور شعبي. الهدف من هذه المناسبة هو تعزيز التعارف والتعبير عن امتنان الشركة لجميع الموظفين وتحقيق النجاح من خلال التعاون بين جميع قطاعات الشركة في المملكة."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46622</th>\n",
       "      <td>خلال العقد الأول من العقد الأول من القرن الحاد...</td>\n",
       "      <td>في العقد الأول من القرن الحادي والعشرين، كان ه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9159</th>\n",
       "      <td>بعض أنصار صباحي يعارضون ترشحه في حالة ترشح الس...</td>\n",
       "      <td>صباحي يؤيد ترشحه في الانتخابات الرئاسية المقبل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73902</th>\n",
       "      <td>كان العميد محمد عتمان، مدير مباحث التموين بالغ...</td>\n",
       "      <td>ضبطت الحملة في الغربية مخالفات تشمل: البيع بأز...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26781</th>\n",
       "      <td>رد فعل الجمهور تجاه حكايات بنات دفع منتج المسل...</td>\n",
       "      <td>رد فعل الجمهور تجاه حكايات بنات دفع منتج المسل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40492</th>\n",
       "      <td>هذا وقد اقيم الحفل في كل من الرياض والظهران وا...</td>\n",
       "      <td>تم تنظيم حفل في الرياض والظهران والجبيل وتبوك ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "46622  خلال العقد الأول من العقد الأول من القرن الحاد...   \n",
       "9159   بعض أنصار صباحي يعارضون ترشحه في حالة ترشح الس...   \n",
       "73902  كان العميد محمد عتمان، مدير مباحث التموين بالغ...   \n",
       "26781  رد فعل الجمهور تجاه حكايات بنات دفع منتج المسل...   \n",
       "40492  هذا وقد اقيم الحفل في كل من الرياض والظهران وا...   \n",
       "\n",
       "                                                 summary  \n",
       "46622  في العقد الأول من القرن الحادي والعشرين، كان ه...  \n",
       "9159   صباحي يؤيد ترشحه في الانتخابات الرئاسية المقبل...  \n",
       "73902  ضبطت الحملة في الغربية مخالفات تشمل: البيع بأز...  \n",
       "26781  رد فعل الجمهور تجاه حكايات بنات دفع منتج المسل...  \n",
       "40492  تم تنظيم حفل في الرياض والظهران والجبيل وتبوك ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "        'text': dataset['train']['text'],\n",
    "        'summary': dataset['train']['summary']\n",
    "    })\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يمكن تصور السلوكيات المُهَدِدة باعتبارها ثمرة لعدم القدرة على التأقلم مع الدافع الطبيعي التنافسي المتعلق بعلاقات الهيمنة المتبادلة التي تلاحظ عامة بين الحيوانات. بدلاً من ذلك، قد ينتج الترهيب في مجتمع من نوع يكون أفراده اجتماعيين، فالبشر بشكل عام يترددون في الدخول في مواجهة أو تهديد عنيف.وهو سلوك مثله مثل جميع السمات السلوكية يظهر بشكل أزيد أو أقل في كل فرد مع مرور الزمن، ولكنه قد يكون «سلوك تعويضي» ذو أهمية كبيرة بالنسبة للبعض مقارنة بالآخرين. فإن المنظرين السلوكيين كثيراً ما يرون أن السلوكيات المُهَدِدة هي نتيجة لتعرض القائمين بها للتهديد من قبل الآخرين، بما في ذلك الآباء، ورموز السلطة، والرفاق والأشقاء. «استخدام القوة مبرر عندما يعتقد الشخص بشكل منطقي أنها ضرورية للدفاع عن النفس أو الآخرين تجاه الاستخدام الفوري لقوة غير شرعية».و قد يتم استخدام الترهيب بوعي أو بغير وعي، ونسبة من الأشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك نتيجة أفكار مستوعبة بأنانية عن تخصيصه لغرض، أو لفائدة أو للتمكين الذاتي. الترهيب المتصل بالتحامل والتمييز يمكن أن يشمل السلوك «الذي يزعج، يهدد، يرهب، وينذر، أو يضع الشخص في حالة خوف على سلامته... بسبب اعتقاد أو تصور بشأن عرقه أو لونه أو أصله القومي أو نسبه أو جنسه أو دينه أو ممارسته لشعائر دينية أو سنه أو إعاقته أو توجهه الجنسي، بغض النظر عن ما إذا كان ذلك الاعتقاد أو التصور صحيحاً».قد يتجلى الترهيب بطرق مختلفة مثل الاحتكاك البدني، أو تجهم الطلعة، أو التلاعب بالمشاعر، أو الإساءة اللفظية، أو جعل شخصاً يشعر بأنه أقل منك، أو الإحراج المتعمد و/أو الاعتداء الجسدي الصريح. «السلوك قد يشمل، ولكنه ليس محصوراً في، الألقاب، التعليقات والافتراءات المهينة والمقترحات البذيئة والاعتداء والإعاقة وعرقلة أو منع الحركة، واللمس الجارح أو أي تداخل جسدي في الحركة أو العمل العادي، والإهانات المرئية، مثل الملصقات أو الرسوم المهينة».ليس هناك تعريف قانوني في القانون الإنجليزي بشأن ما يشكله سلوك «الترهيب» Intimidation، ولذا فالأمر متروك للمحاكم لاتخاذ قرارها بشأن كل حالة على حدة. ومع ذلك، إذا هدد شخص بالعنف تجاه شخص آخر، فان ذلك قد يشكل جريمة جنائية.\n",
      "1876\n"
     ]
    }
   ],
   "source": [
    "print( df['text'][0] )\n",
    "print(len(df[\"text\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "السلوكيات المهددة يمكن أن تكون نتيجة عدم القدرة على التأقلم مع العلاقات الهيمنة التنافسية المتبادلة بين الحيوانات. قد ينتج الترهيب في مجتمع من النوع الاجتماعي. الترهيب يتم استخدامه بوعي أو بغير وعي، ونسبة الأشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب أفكار أنانية. الترهيب يمكن أن يشمل الاحتكاك البدني، أو التلاعب بالمشاعر، أو الإساءة اللفظية، أو الإحراج المتعمد و/أو الاعتداء الجسدي. قد يشكل تهديد العنف جريمة جنائية.\n",
      "422\n"
     ]
    }
   ],
   "source": [
    "print( df['summary'][0] )\n",
    "print(len(df[\"summary\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ء',\n",
       " 'ءَ',\n",
       " 'آ',\n",
       " 'آب',\n",
       " 'آذار',\n",
       " 'آض',\n",
       " 'آمينَ',\n",
       " 'آناء',\n",
       " 'آنفا',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'آهاً',\n",
       " 'آهٍ',\n",
       " 'آهِ',\n",
       " 'آي',\n",
       " 'أ',\n",
       " 'أبدا',\n",
       " 'أبريل',\n",
       " 'أبو',\n",
       " 'أبٌ',\n",
       " 'أجل',\n",
       " 'أجمع',\n",
       " 'أحد',\n",
       " 'أخبر',\n",
       " 'أخذ',\n",
       " 'أخو',\n",
       " 'أخٌ',\n",
       " 'أربع',\n",
       " 'أربعاء',\n",
       " 'أربعة',\n",
       " 'أربعمئة',\n",
       " 'أربعمائة',\n",
       " 'أرى',\n",
       " 'أسكن',\n",
       " 'أصبح',\n",
       " 'أصلا',\n",
       " 'أضحى',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'أعلم',\n",
       " 'أغسطس',\n",
       " 'أف',\n",
       " 'أفريل',\n",
       " 'أفعل به',\n",
       " 'أفٍّ',\n",
       " 'أقبل',\n",
       " 'أقل',\n",
       " 'أكتوبر',\n",
       " 'أكثر',\n",
       " 'أل',\n",
       " 'ألا',\n",
       " 'ألف',\n",
       " 'ألفى',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أمام',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'أمسى',\n",
       " 'أمّا',\n",
       " 'أن',\n",
       " 'أنا',\n",
       " 'أنبأ',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'أنتِ',\n",
       " 'أنشأ',\n",
       " 'أنى',\n",
       " 'أنًّ',\n",
       " 'أنّى',\n",
       " 'أهلا',\n",
       " 'أو',\n",
       " 'أوت',\n",
       " 'أوشك',\n",
       " 'أول',\n",
       " 'أولئك',\n",
       " 'أولاء',\n",
       " 'أولالك',\n",
       " 'أوه',\n",
       " 'أوّهْ',\n",
       " 'أى',\n",
       " 'أي',\n",
       " 'أيا',\n",
       " 'أيار',\n",
       " 'أيضا',\n",
       " 'أيلول',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'أيها',\n",
       " 'أيّ',\n",
       " 'أيّان',\n",
       " 'أُفٍّ',\n",
       " 'ؤ',\n",
       " 'إحدى',\n",
       " 'إذ',\n",
       " 'إذا',\n",
       " 'إذاً',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'إزاء',\n",
       " 'إلا',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'إليكنّ',\n",
       " 'إليكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إلّا',\n",
       " 'إما',\n",
       " 'إمّا',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'إنَّ',\n",
       " 'إى',\n",
       " 'إي',\n",
       " 'إياك',\n",
       " 'إياكم',\n",
       " 'إياكما',\n",
       " 'إياكن',\n",
       " 'إيانا',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهم',\n",
       " 'إياهما',\n",
       " 'إياهن',\n",
       " 'إياي',\n",
       " 'إيه',\n",
       " 'إيهٍ',\n",
       " 'ئ',\n",
       " 'ا',\n",
       " 'ابتدأ',\n",
       " 'اتخذ',\n",
       " 'اثنا',\n",
       " 'اثنان',\n",
       " 'اثني',\n",
       " 'اثنين',\n",
       " 'اخلولق',\n",
       " 'اربعون',\n",
       " 'اربعين',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'الآن',\n",
       " 'الألاء',\n",
       " 'الألى',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللائي',\n",
       " 'اللاتي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'انبرى',\n",
       " 'انقلب',\n",
       " 'ب',\n",
       " 'بؤسا',\n",
       " 'بئس',\n",
       " 'باء',\n",
       " 'بات',\n",
       " 'بخ',\n",
       " 'بخٍ',\n",
       " 'بس',\n",
       " 'بسّ',\n",
       " 'بضع',\n",
       " 'بطآن',\n",
       " 'بعد',\n",
       " 'بعدا',\n",
       " 'بعض',\n",
       " 'بغتة',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بيد',\n",
       " 'بين',\n",
       " 'بَسْ',\n",
       " 'بَلْهَ',\n",
       " 'ة',\n",
       " 'ت',\n",
       " 'تاء',\n",
       " 'تارة',\n",
       " 'تاسع',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تبدّل',\n",
       " 'تجاه',\n",
       " 'تحت',\n",
       " 'تحوّل',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تسع',\n",
       " 'تسعة',\n",
       " 'تسعمئة',\n",
       " 'تسعمائة',\n",
       " 'تسعون',\n",
       " 'تسعين',\n",
       " 'تشرين',\n",
       " 'تعسا',\n",
       " 'تعلَّم',\n",
       " 'تفعلان',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'تلقاء',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'تموز',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'تَيْنِ',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'ث',\n",
       " 'ثاء',\n",
       " 'ثالث',\n",
       " 'ثامن',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثلاث',\n",
       " 'ثلاثاء',\n",
       " 'ثلاثة',\n",
       " 'ثلاثمئة',\n",
       " 'ثلاثمائة',\n",
       " 'ثلاثون',\n",
       " 'ثلاثين',\n",
       " 'ثم',\n",
       " 'ثمان',\n",
       " 'ثمانمئة',\n",
       " 'ثمانون',\n",
       " 'ثماني',\n",
       " 'ثمانية',\n",
       " 'ثمانين',\n",
       " 'ثمة',\n",
       " 'ثمنمئة',\n",
       " 'ثمَّ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ج',\n",
       " 'جانفي',\n",
       " 'جعل',\n",
       " 'جلل',\n",
       " 'جمعة',\n",
       " 'جميع',\n",
       " 'جنيه',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'جير',\n",
       " 'جيم',\n",
       " 'ح',\n",
       " 'حاء',\n",
       " 'حادي',\n",
       " 'حار',\n",
       " 'حاشا',\n",
       " 'حاي',\n",
       " 'حبذا',\n",
       " 'حبيب',\n",
       " 'حتى',\n",
       " 'حجا',\n",
       " 'حدَث',\n",
       " 'حرى',\n",
       " 'حزيران',\n",
       " 'حسب',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'حمو',\n",
       " 'حمٌ',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'حيَّ',\n",
       " 'حَذارِ',\n",
       " 'خ',\n",
       " 'خاء',\n",
       " 'خاصة',\n",
       " 'خال',\n",
       " 'خامس',\n",
       " 'خبَّر',\n",
       " 'خلا',\n",
       " 'خلافا',\n",
       " 'خلف',\n",
       " 'خمس',\n",
       " 'خمسة',\n",
       " 'خمسمئة',\n",
       " 'خمسمائة',\n",
       " 'خمسون',\n",
       " 'خمسين',\n",
       " 'خميس',\n",
       " 'د',\n",
       " 'دال',\n",
       " 'درهم',\n",
       " 'درى',\n",
       " 'دواليك',\n",
       " 'دولار',\n",
       " 'دون',\n",
       " 'دونك',\n",
       " 'ديسمبر',\n",
       " 'دينار',\n",
       " 'ذ',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذال',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذانِ',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذهب',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذيت',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ذَيْنِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ر',\n",
       " 'رأى',\n",
       " 'راء',\n",
       " 'رابع',\n",
       " 'راح',\n",
       " 'رجع',\n",
       " 'رزق',\n",
       " 'رويدك',\n",
       " 'ريال',\n",
       " 'ريث',\n",
       " 'رُبَّ',\n",
       " 'ز',\n",
       " 'زاي',\n",
       " 'زعم',\n",
       " 'زود',\n",
       " 'س',\n",
       " 'ساء',\n",
       " 'سابع',\n",
       " 'سادس',\n",
       " 'سبت',\n",
       " 'سبتمبر',\n",
       " 'سبحان',\n",
       " 'سبع',\n",
       " 'سبعة',\n",
       " 'سبعمئة',\n",
       " 'سبعمائة',\n",
       " 'سبعون',\n",
       " 'سبعين',\n",
       " 'ست',\n",
       " 'ستة',\n",
       " 'ستمئة',\n",
       " 'ستمائة',\n",
       " 'ستون',\n",
       " 'ستين',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سرعان',\n",
       " 'سقى',\n",
       " 'سمعا',\n",
       " 'سنتيم',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'سين',\n",
       " 'ش',\n",
       " 'شباط',\n",
       " 'شبه',\n",
       " 'شتان',\n",
       " 'شتانَ',\n",
       " 'شرع',\n",
       " 'شمال',\n",
       " 'شيكل',\n",
       " 'شين',\n",
       " 'شَتَّانَ',\n",
       " 'ص',\n",
       " 'صاد',\n",
       " 'صار',\n",
       " 'صباح',\n",
       " 'صبر',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'صهٍ',\n",
       " 'صهْ',\n",
       " 'ض',\n",
       " 'ضاد',\n",
       " 'ضحوة',\n",
       " 'ط',\n",
       " 'طاء',\n",
       " 'طاق',\n",
       " 'طالما',\n",
       " 'طرا',\n",
       " 'طفق',\n",
       " 'طَق',\n",
       " 'ظ',\n",
       " 'ظاء',\n",
       " 'ظلّ',\n",
       " 'ظنَّ',\n",
       " 'ع',\n",
       " 'عاد',\n",
       " 'عاشر',\n",
       " 'عامة',\n",
       " 'عجبا',\n",
       " 'عدا',\n",
       " 'عدَّ',\n",
       " 'عسى',\n",
       " 'عشر',\n",
       " 'عشرة',\n",
       " 'عشرون',\n",
       " 'عشرين',\n",
       " 'عل',\n",
       " 'علق',\n",
       " 'علم',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'علًّ',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'عوض',\n",
       " 'عيانا',\n",
       " 'عين',\n",
       " 'عَدَسْ',\n",
       " 'غ',\n",
       " 'غادر',\n",
       " 'غالبا',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'غير',\n",
       " 'غين',\n",
       " 'ف',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فاء',\n",
       " 'فبراير',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'فلا',\n",
       " 'فلان',\n",
       " 'فلس',\n",
       " 'فمن',\n",
       " 'فو',\n",
       " 'فوق',\n",
       " 'في',\n",
       " 'فيفري',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'ق',\n",
       " 'قاطبة',\n",
       " 'قاف',\n",
       " 'قام',\n",
       " 'قبل',\n",
       " 'قد',\n",
       " 'قرش',\n",
       " 'قطّ',\n",
       " 'قلما',\n",
       " 'ك',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأنّ',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'كاد',\n",
       " 'كاف',\n",
       " 'كان',\n",
       " 'كانون',\n",
       " 'كثيرا',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كرب',\n",
       " 'كسا',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كلَّا',\n",
       " 'كلّما',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كن',\n",
       " 'كى',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'كِخ',\n",
       " 'ل',\n",
       " 'لئن',\n",
       " 'لا',\n",
       " 'لا سيما',\n",
       " 'لات',\n",
       " 'لاسيما',\n",
       " 'لام',\n",
       " 'لبيك',\n",
       " 'لدن',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لعلَّ',\n",
       " 'لعمر',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكنَّ',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لمّا',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'ليت',\n",
       " 'ليرة',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'م',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ما',\n",
       " 'ما أفعله',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مائة',\n",
       " 'مادام',\n",
       " 'ماذا',\n",
       " 'مارس',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ماي',\n",
       " 'مايو',\n",
       " 'متى',\n",
       " 'مثل',\n",
       " 'مذ',\n",
       " 'مرّة',\n",
       " 'مساء',\n",
       " 'مع',\n",
       " 'معاذ',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'مكانَك',\n",
       " 'مليم',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منذ',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'ميم',\n",
       " 'ن',\n",
       " 'نا',\n",
       " 'نبَّا',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'نفس',\n",
       " 'نوفمبر',\n",
       " 'نون',\n",
       " 'نيسان',\n",
       " 'نيف',\n",
       " 'نَخْ',\n",
       " 'نَّ',\n",
       " 'ه',\n",
       " 'هؤلاء',\n",
       " 'ها',\n",
       " 'هاء',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاكَ',\n",
       " 'هاهنا',\n",
       " 'هبّ',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هللة',\n",
       " 'هلم',\n",
       " 'هلّا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'همزة',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'هيّا',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَجْ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذَيْنِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَيْهات',\n",
       " 'و',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'وا',\n",
       " 'واحد',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'واهاً',\n",
       " 'واو',\n",
       " 'وجد',\n",
       " 'وراءَك',\n",
       " 'ورد',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهب',\n",
       " 'وهو',\n",
       " 'وَيْ',\n",
       " 'وُشْكَانَ',\n",
       " 'ى',\n",
       " 'ي',\n",
       " 'يا',\n",
       " 'ياء',\n",
       " 'يفعلان',\n",
       " 'يفعلون',\n",
       " 'يمين',\n",
       " 'ين',\n",
       " 'يناير',\n",
       " 'يوان',\n",
       " 'يورو',\n",
       " 'يوليو',\n",
       " 'يونيو',\n",
       " 'ّأيّان'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_stopwords = set(stopwords.words('arabic')) \n",
    "arabic_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_patterns = {\n",
    "            # Remove diacritics\n",
    "            r'[\\u064B-\\u065F\\u0670]': '',\n",
    "            # Normalize Arabic characters\n",
    "            r'[إأآا]': 'ا',\n",
    "            r'ى': 'ي',\n",
    "            r'ة': 'ه'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "        for pattern, replacement in arabic_patterns.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the words \n",
    "the Arabic stop words we load earlier and the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stopwords(stopwords_set):\n",
    "    cleaned_stopwords = set()\n",
    "    for word in stopwords_set:\n",
    "        # normalize them \n",
    "        word = normalize_arabic(word)\n",
    "        cleaned_stopwords.add(word)\n",
    "    return cleaned_stopwords\n",
    "\n",
    "# cleaned set \n",
    "arabic_stopwords = clean_stopwords(arabic_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        # remove whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # remove  urls\n",
    "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "        # remove emails\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "        arabic_punctuation = '،؛؟'\n",
    "        # remove other punctuation \n",
    "        punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        \n",
    "        punctuation_to_remove = ''.join(ch for ch in punctuation if ch not in arabic_punctuation)\n",
    "        return text.translate(str.maketrans('', '', punctuation_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ara_tokenizer(text):\n",
    "        text = normalize_arabic(text)\n",
    "        tokens = re.findall(r'\\w+', text)\n",
    "        return [token for token in tokens if token not in arabic_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will extract the basic features for any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\n",
    "            'length': 0,\n",
    "            'word_count': 0,\n",
    "            'avg_word_length': 0,\n",
    "            'lexical_diversity': 0\n",
    "        }\n",
    "    \n",
    "    text = normalize_arabic(text)  \n",
    "    tokens = ara_tokenizer(text)\n",
    "\n",
    "    features = {\n",
    "        'length': len(text),\n",
    "        'word_count': len(tokens),\n",
    "        'avg_word_length': sum(len(word) for word in tokens) / max(1, len(tokens)),\n",
    "        'lexical_diversity': len(set(tokens)) / max(1, len(tokens))\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining all the function in one to implement preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "        # check if its a text \n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "            \n",
    "        text = str(text).strip()\n",
    "        \n",
    "        text = clean_text(text)\n",
    "        text = remove_punctuation(text)\n",
    "        text = normalize_arabic(text)\n",
    "            \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just an extra step to implement cosain similarty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=500, \n",
    "            ngram_range=(1, 2),\n",
    "            tokenizer= lambda text: ara_tokenizer(text),\n",
    "            stop_words=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'يمكن تصور السلوكيات المهدده باعتبارها ثمره لعدم القدره علي التاقلم مع الدافع الطبيعي التنافسي المتعلق بعلاقات الهيمنه المتبادله التي تلاحظ عامه بين الحيوانات بدلا من ذلك، قد ينتج الترهيب في مجتمع من نوع يكون افراده اجتماعيين، فالبشر بشكل عام يترددون في الدخول في مواجهه او تهديد عنيفوهو سلوك مثله مثل جميع السمات السلوكيه يظهر بشكل ازيد او اقل في كل فرد مع مرور الزمن، ولكنه قد يكون «سلوك تعويضي» ذو اهميه كبيره بالنسبه للبعض مقارنه بالاخرين فان المنظرين السلوكيين كثيرا ما يرون ان السلوكيات المهدده هي نتيجه لتعرض القائمين بها للتهديد من قبل الاخرين، بما في ذلك الاباء، ورموز السلطه، والرفاق والاشقاء «استخدام القوه مبرر عندما يعتقد الشخص بشكل منطقي انها ضروريه للدفاع عن النفس او الاخرين تجاه الاستخدام الفوري لقوه غير شرعيه»و قد يتم استخدام الترهيب بوعي او بغير وعي، ونسبه من الاشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك نتيجه افكار مستوعبه بانانيه عن تخصيصه لغرض، او لفائده او للتمكين الذاتي الترهيب المتصل بالتحامل والتمييز يمكن ان يشمل السلوك «الذي يزعج، يهدد، يرهب، وينذر، او يضع الشخص في حاله خوف علي سلامته بسبب اعتقاد او تصور بشان عرقه او لونه او اصله القومي او نسبه او جنسه او دينه او ممارسته لشعائر دينيه او سنه او اعاقته او توجهه الجنسي، بغض النظر عن ما اذا كان ذلك الاعتقاد او التصور صحيحا»قد يتجلي الترهيب بطرق مختلفه مثل الاحتكاك البدني، او تجهم الطلعه، او التلاعب بالمشاعر، او الاساءه اللفظيه، او جعل شخصا يشعر بانه اقل منك، او الاحراج المتعمد واو الاعتداء الجسدي الصريح «السلوك قد يشمل، ولكنه ليس محصورا في، الالقاب، التعليقات والافتراءات المهينه والمقترحات البذيئه والاعتداء والاعاقه وعرقله او منع الحركه، واللمس الجارح او اي تداخل جسدي في الحركه او العمل العادي، والاهانات المرئيه، مثل الملصقات او الرسوم المهينه»ليس هناك تعريف قانوني في القانون الانجليزي بشان ما يشكله سلوك «الترهيب» Intimidation، ولذا فالامر متروك للمحاكم لاتخاذ قرارها بشان كل حاله علي حده ومع ذلك، اذا هدد شخص بالعنف تجاه شخص اخر، فان ذلك قد يشكل جريمه جنائيه'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing to df text and summary \n",
    "df['text'] = df['text'].apply(preprocess_text) \n",
    "df['summary'] = df['summary'].apply(preprocess_text)  \n",
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'السلوكيات المهدده يمكن ان تكون نتيجه عدم القدره علي التاقلم مع العلاقات الهيمنه التنافسيه المتبادله بين الحيوانات قد ينتج الترهيب في مجتمع من النوع الاجتماعي الترهيب يتم استخدامه بوعي او بغير وعي، ونسبه الاشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب افكار انانيه الترهيب يمكن ان يشمل الاحتكاك البدني، او التلاعب بالمشاعر، او الاساءه اللفظيه، او الاحراج المتعمد واو الاعتداء الجسدي قد يشكل تهديد العنف جريمه جنائيه'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison\n",
    "before=\"السلوكيات المهددة يمكن أن تكون نتيجة عدم القدرة على التأقلم مع العلاقات الهيمنة التنافسية المتبادلة بين الحيوانات. قد ينتج الترهيب في مجتمع من النوع الاجتماعي. الترهيب يتم استخدامه بوعي أو بغير وعي، ونسبة الأشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب أفكار أنانية. الترهيب يمكن أن يشمل الاحتكاك البدني، أو التلاعب بالمشاعر، أو الإساءة اللفظية، أو الإحراج المتعمد و/أو الاعتداء الجسدي. قد يشكل تهديد العنف جريمة جنائية\"\n",
    "after =\"السلوكيات المهدده يمكن ان تكون نتيجه عدم القدره علي التاقلم مع العلاقات الهيمنه التنافسيه المتبادله بين الحيوانات قد ينتج الترهيب في مجتمع من النوع الاجتماعي الترهيب يتم استخدامه بوعي او بغير وعي، ونسبه الاشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب افكار انانيه الترهيب يمكن ان يشمل الاحتكاك البدني، او التلاعب بالمشاعر، او الاساءه اللفظيه، او الاحراج المتعمد واو الاعتداء الجسدي قد يشكل تهديد العنف جريمه جنائيه\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features\n",
    "text_features = pd.DataFrame(df['text'].apply(extract_features).tolist())\n",
    "text_features.columns = [f'text_{col}' for col in text_features.columns]\n",
    "\n",
    "summary_features = pd.DataFrame(df['summary'].apply(extract_features).tolist())\n",
    "summary_features.columns = [f'summary_{col}' for col in summary_features.columns]\n",
    "\n",
    "# combine the data with the extracted features by column\n",
    "df_with_features = pd.concat([df, text_features, summary_features], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_avg_word_length",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_lexical_diversity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "summary_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_avg_word_length",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "summary_lexical_diversity",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "dce06ec5-5085-4336-8270-a82cb9848948",
       "rows": [
        [
         "0",
         "يمكن تصور السلوكيات المهدده باعتبارها ثمره لعدم القدره علي التاقلم مع الدافع الطبيعي التنافسي المتعلق بعلاقات الهيمنه المتبادله التي تلاحظ عامه بين الحيوانات بدلا من ذلك، قد ينتج الترهيب في مجتمع من نوع يكون افراده اجتماعيين، فالبشر بشكل عام يترددون في الدخول في مواجهه او تهديد عنيفوهو سلوك مثله مثل جميع السمات السلوكيه يظهر بشكل ازيد او اقل في كل فرد مع مرور الزمن، ولكنه قد يكون «سلوك تعويضي» ذو اهميه كبيره بالنسبه للبعض مقارنه بالاخرين فان المنظرين السلوكيين كثيرا ما يرون ان السلوكيات المهدده هي نتيجه لتعرض القائمين بها للتهديد من قبل الاخرين، بما في ذلك الاباء، ورموز السلطه، والرفاق والاشقاء «استخدام القوه مبرر عندما يعتقد الشخص بشكل منطقي انها ضروريه للدفاع عن النفس او الاخرين تجاه الاستخدام الفوري لقوه غير شرعيه»و قد يتم استخدام الترهيب بوعي او بغير وعي، ونسبه من الاشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك نتيجه افكار مستوعبه بانانيه عن تخصيصه لغرض، او لفائده او للتمكين الذاتي الترهيب المتصل بالتحامل والتمييز يمكن ان يشمل السلوك «الذي يزعج، يهدد، يرهب، وينذر، او يضع الشخص في حاله خوف علي سلامته بسبب اعتقاد او تصور بشان عرقه او لونه او اصله القومي او نسبه او جنسه او دينه او ممارسته لشعائر دينيه او سنه او اعاقته او توجهه الجنسي، بغض النظر عن ما اذا كان ذلك الاعتقاد او التصور صحيحا»قد يتجلي الترهيب بطرق مختلفه مثل الاحتكاك البدني، او تجهم الطلعه، او التلاعب بالمشاعر، او الاساءه اللفظيه، او جعل شخصا يشعر بانه اقل منك، او الاحراج المتعمد واو الاعتداء الجسدي الصريح «السلوك قد يشمل، ولكنه ليس محصورا في، الالقاب، التعليقات والافتراءات المهينه والمقترحات البذيئه والاعتداء والاعاقه وعرقله او منع الحركه، واللمس الجارح او اي تداخل جسدي في الحركه او العمل العادي، والاهانات المرئيه، مثل الملصقات او الرسوم المهينه»ليس هناك تعريف قانوني في القانون الانجليزي بشان ما يشكله سلوك «الترهيب» Intimidation، ولذا فالامر متروك للمحاكم لاتخاذ قرارها بشان كل حاله علي حده ومع ذلك، اذا هدد شخص بالعنف تجاه شخص اخر، فان ذلك قد يشكل جريمه جنائيه",
         "السلوكيات المهدده يمكن ان تكون نتيجه عدم القدره علي التاقلم مع العلاقات الهيمنه التنافسيه المتبادله بين الحيوانات قد ينتج الترهيب في مجتمع من النوع الاجتماعي الترهيب يتم استخدامه بوعي او بغير وعي، ونسبه الاشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب افكار انانيه الترهيب يمكن ان يشمل الاحتكاك البدني، او التلاعب بالمشاعر، او الاساءه اللفظيه، او الاحراج المتعمد واو الاعتداء الجسدي قد يشكل تهديد العنف جريمه جنائيه",
         "1850",
         "220",
         "5.663636363636364",
         "0.8772727272727273",
         "416",
         "50",
         "6.02",
         "0.92"
        ],
        [
         "1",
         "يقدر المؤرخون مجموع عدد الرقيق خلال الفتره الممتده بدءا من مطلع التاريخ الاسلامي في عام 650 وحتي الغاء الاتجار بالرقيق في شبه الجزيره العربيه في منتصف القرن العشرين بما يتراوح عدده من 10 الي 18 مليون شخص كان يطلق عليهم اسم الزنج كان تجار الرقيق من شرق افريقيا يستجلبون هؤلاء العبيد وينقلوهم الي شبه الجزيره العربيه وغيرها من المناطق المجاوره للبيع في اسواق النخاسه كانت اعداد الرقيق الذين استجلبوا الي المنطقه من خلال هذه الممارسه اكثر بكثير من العبيد المنقولين الي الامريكيتين اثرت عده عوامل علي عدم بروز اسلاف هذا المكون السكاني في المجتمعات العربيه المعاصره خلال القرن الحادي والعشرين اولها ان معظم هؤلاء العبيد كانوا جوار من النساء حيث كان الطلب علي شرائهن مرتفعا في شبه الجزيره العربيه والمناطق المجاوره حيث خدمن كمحظيات اما العبيد من الرجال فكان يتم اخصائهم حتي يخدموا كحرس يلبون حريم اسيادهم هذا وقد كان معدل وفيات الرقيق السود الذين اشتغلوا بالسخره عاليا الي حد ملحوظ كان الاطفال الذين ولدوا من تزاوج الساده العرب مع الجواري من ذوي اصول عرقيه مختلطه وكان هؤلاء يدمجون مع عائلات ابائهم ويتمتعون بالحريه علي عكس امهاتهم، وذلك نظرا لطبيعه المجتمع الذي كان افراده ينسبون حصرا الي ابائهم ولذلك لم يبقي من مجتمعات العرب الافارقه المنحدره من اسلاف هؤلاء الرقيق في شبه الجزيره العربيه والدول المجاوره سوي القليليوجد في عدد من البلدان الشرق اوسطيه عده اقليات عربيه من ذوي اصول افريقيه مثل العراق التي فيها نحو 12 مليون عراقي اسود اشتكت هذه المجتمعات من تعرضها لتاريخ حافل من التمييز والعنصريه ومع ذلك فقد بذل الافروعراقيون المنحدرون من طبقه «الزنج» جهودا حثيثه من اجل نيل اعتراف حكومي بوضعهم كاقليه مما سيمكنهم من الحصول علي مقاعد تمثيليه لهم في البرلمان كغيرهم من فئات الشعب العراقي تعتبر معظم هذه المجتمعات المنتشره في المنطقه انفسها كعرب وافارقه وفقا للباحث علمين مزروي وزملائه",
         "يقدر عدد الرقيق خلال الفتره الممتده من 650 الي منتصف القرن العشرين بين 10 الي 18 مليون شخص كان تجار الرقيق يستجلبون العبيد من شرق افريقيا وينقلونهم الي شبه الجزيره العربيه وغيرها للبيع في اسواق النخاسه لم يبق من مجتمعات العرب الافارقه المنحدره من اسلاف هؤلاء الرقيق سوي القليل يوجد في عدد من البلدان الشرق اوسطيه اقليات عربيه من ذوي اصول افريقيه، مثل العراق، حيث يشتكي هؤلاء المجتمعات من التمييز والعنصريه الافروعراقيون يسعون لنيل اعتراف حكومي كاقليه للحصول علي مقاعد تمثيليه في البرلمان",
         "1679",
         "206",
         "5.62621359223301",
         "0.8446601941747572",
         "487",
         "60",
         "5.683333333333334",
         "0.95"
        ],
        [
         "2",
         "الرياض قامت صباح امس السبت بجوله في سوق السبت بالحليله احد الاسواق الشعبيه ورصدت التذمر الكبير جراء الارتفاع المخيف في الاسعار، فقد اشار يوسف احد الباعه ان مرحله الباذنجان ارتفعت يوم امس فقط عن السعر قبل يومين فقد ارتفعت من الي ريالا، واكد يوسف ان الحال ذاته ينطبق علي الجزر والكوسه والباميا والبطاطس والبصل والخس فقد تضاعفت الي الضعف قبل دخول رمضان فيما تضاعفت اكثر من الضعفين عن سعرها في رمضان السنه الماضيه، فيما اورد البائع راضي مثالا علي الخيار فقد كان يباع الكرتون منه في بدايه رمضان السنه الماضيه بخمسه ريالات فيما يباع اليوم امس بعشرين ريالا، فيما كانت تباع الباميا بعشره ريالات والان تباع بثلاثين ريالا كما يباع صندوق الطماطم المستورد من تركيا ب ريالا فيما كان بسبعه ريالات بعض الموردين برر في حديث له مع الرياض سبب الارتفاع الي الحرب في لبنان مذكرا ان لبنان احد المصدرين الرئيسيين للخضار والفاكهه للمملكه، كما عزوا الارتفاع الي ارتفاع حراره الجو هذا العام مبينين ان رمضان في العام الماضي كان الطقس فيه افضل بكثير من هذا العام مما اثر في ضعف الانتاج المحلي",
         "قامت الرياض صباح امس السبت بجوله في سوق السبت بالحليله ولاحظت ارتفاعا كبيرا في اسعار الباذنجان، الجزر، الكوسه، الباميا، البطاطس، البصل، والخس وتضاعفت الاسعار قبل رمضان باكثر من الضعفين مقارنه بالعام الماضي وبرر بعض الموردين الارتفاع بسبب الحرب في لبنان وارتفاع حراره الجو هذا العام",
         "965",
         "120",
         "5.366666666666666",
         "0.7833333333333333",
         "281",
         "36",
         "5.75",
         "0.9722222222222222"
        ],
        [
         "3",
         "كان السميلودون اخر اجناس السنوريات سيفيه الانياب التي عاشت علي وجه الارض، وهو ايضا اشهرها، ويبلغ من مقدار شهرته ان عامه الناس تعتقد بان تسميتي «السنور سيفي الانياب» و«السميلودون» مترادفتان وتعنيان شيئا واحدا، علي الرغم من ان الاولي اسم عام يستخدم في وصف عده اجناس سنوريات بائده تمتعت بانياب علويه فائقه الطول ويلاحظ ان معظم تلك الاجناس تطورت ونشات تقاربيا نتيجه تشابه ظروف معيشتها، دون ان تكون وثيقه الصله ببعضها بالضروره ويلاحظ ايضا ان حيوانات اخري من غير السنوريات ظهرت لديها تلك الميزه كونها عاشت حياه شبيهه بحياه السنوريات سيفيه الانياب من ابرز تلك الحيوانات غورغونيات الوجه الاسم العلمي Gorgonopsia والثايلاكوسميليدات الاسم العلمي Thylacosmilidae وخنجريات الاسنان الاسم العلمي Machaeroides والنمرڤيديات الاسم العلمي Nimravidae والباربروفيلسات الاسم العلمي Barbourofelidae وسيفيات الاسنان الاسم العلمي Machairodontinae تشتمل فصيله السنوريات الحقيقيه علي سيفيات الاسنان بصفتها فصيله ضمنها، فهي تصنيف فرعي ادني منها، والاخيره تقسم بدورها الي ثلاث قبائل سيفيات الاسنان المزيفه الاسم العلمي Metailurini، والسنوريات حرابيه الاسنان الاسم العلمي Homotherini، والسنوريات طعانه الاسنان الاسم العلمي Smilodontini، والقبيله الاخيره هي ما ينتمي اليها جنس السميلودون تتميز السنوريات المنتميه لقبيله طعانه الاسنان بانيابها الطويله النحيله المسننه، او عديمه التسنين في بعض الحالات، بالمقابل تتميز السنوريات حرابيه الاسنان بانياب اقصر واعرض واشد تفلطحا واخشن تسنينا اما السنوريات المنتميه الي قبيله سيفيات الاسنان المزيفه، فكانت اقل تخصصا، وانيابها اقصر واقل تفلطحا، حتي ان بعض الباحثين يخرجونها من نطاق فصيله سيفيات الاسنان لافتقادها لما يميز اعضاء هذه الفصيله",
         "السميلودون هو اخر اجناس السنوريات سيفيه الانياب، وهو اشهرها تسميتي السنور سيفي الانياب والسميلودون ليست مترادفتين تطورت العديد من الاجناس التي تمتعت بانياب علويه فائقه الطول بشكل متقارب تشمل الحيوانات الاخري التي ظهرت لديها تلك الميزه غورغونيات الوجه، الثايلاكوسميليدات، خنجريات الاسنان، النمرفيديات، الباربروفيلسات، وسيفيات الاسنان السميلودون ينتمي الي قبيله سنوريات طعانه الاسنان، ويتميز بانيابه الطويله النحيله المسننه",
         "1549",
         "187",
         "6.363636363636363",
         "0.7165775401069518",
         "421",
         "49",
         "6.795918367346939",
         "0.9183673469387755"
        ],
        [
         "4",
         "وقال سموه في كلمه له بمناسبه اليوم الوطني ان كل هذا لم يكن ليتحقق لولا توفيق الله ثم مواصله ابناء الملك عبدالعزيز رحمه الله لمسيره التنميه والبناء وتطوير هذه البلاد الغاليه والنهوض بها في جميع المجالات حتي وصلنا الي هذا العهد الزاهر بقياده خادم الحرمين الشريفين الملك عبدالله بن عبدالعزيز وسمو ولي العهد الامين وسمو النائب الثاني حفظهم الله واصبحنا نري الانجازات تتحقق في بلادنا علي مختلف الاصعده الاقتصاديه والسياسيه والاجتماعيه ونحن في هذا اليوم نعيش نهضه نماء وتطور بعد مرور عاما علي توحيد بلادنا الغاليه ونحمد الله علي ما انعم به علينا من نعم كثيره بفضله تعالي ثم بدعم ومتابعه ولاه الامر يحفظهم الله وسال سموه الله تعالي ان يديم علي وطننا نعمه الامن والاستقرار في ظل حكومه خادم الحرمين الشريفين الملك عبدالله بن عبدالعزيز وسمو ولي العهد الامين وسمو النائب الثاني حفظهم الله داعيا المولي العلي القدير ان يوفق قيادتنا لما فيه خير امتنا الاسلاميه",
         "لم يكن اليوم الوطني يعتبر تحقيقا لولا مسيره التنميه والبناء والتطوير التي قادها الملك عبدالعزيز رحمه الله واستمرار ابنائه بقياده الملك عبدالله بن عبدالعزيز وسمو ولي العهد الامين وسمو النائب الثاني، شهدت البلاد تقدما اقتصاديا وسياسيا واجتماعيا نحن نشهد نهضه في جميع المجالات بفضل دعم ومتابعه حكومتنا نطلب من الله الامن والاستقرار لوطننا تحت قياده حكومتنا الرشيده",
         "847",
         "113",
         "5.433628318584071",
         "0.7345132743362832",
         "361",
         "49",
         "5.714285714285714",
         "0.8979591836734694"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_avg_word_length</th>\n",
       "      <th>text_lexical_diversity</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>summary_word_count</th>\n",
       "      <th>summary_avg_word_length</th>\n",
       "      <th>summary_lexical_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>يمكن تصور السلوكيات المهدده باعتبارها ثمره لعد...</td>\n",
       "      <td>السلوكيات المهدده يمكن ان تكون نتيجه عدم القدر...</td>\n",
       "      <td>1850</td>\n",
       "      <td>220</td>\n",
       "      <td>5.663636</td>\n",
       "      <td>0.877273</td>\n",
       "      <td>416</td>\n",
       "      <td>50</td>\n",
       "      <td>6.020000</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>يقدر المؤرخون مجموع عدد الرقيق خلال الفتره الم...</td>\n",
       "      <td>يقدر عدد الرقيق خلال الفتره الممتده من 650 الي...</td>\n",
       "      <td>1679</td>\n",
       "      <td>206</td>\n",
       "      <td>5.626214</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>487</td>\n",
       "      <td>60</td>\n",
       "      <td>5.683333</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الرياض قامت صباح امس السبت بجوله في سوق السبت ...</td>\n",
       "      <td>قامت الرياض صباح امس السبت بجوله في سوق السبت ...</td>\n",
       "      <td>965</td>\n",
       "      <td>120</td>\n",
       "      <td>5.366667</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>281</td>\n",
       "      <td>36</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>كان السميلودون اخر اجناس السنوريات سيفيه الاني...</td>\n",
       "      <td>السميلودون هو اخر اجناس السنوريات سيفيه الانيا...</td>\n",
       "      <td>1549</td>\n",
       "      <td>187</td>\n",
       "      <td>6.363636</td>\n",
       "      <td>0.716578</td>\n",
       "      <td>421</td>\n",
       "      <td>49</td>\n",
       "      <td>6.795918</td>\n",
       "      <td>0.918367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>وقال سموه في كلمه له بمناسبه اليوم الوطني ان ك...</td>\n",
       "      <td>لم يكن اليوم الوطني يعتبر تحقيقا لولا مسيره ال...</td>\n",
       "      <td>847</td>\n",
       "      <td>113</td>\n",
       "      <td>5.433628</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>361</td>\n",
       "      <td>49</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>0.897959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  يمكن تصور السلوكيات المهدده باعتبارها ثمره لعد...   \n",
       "1  يقدر المؤرخون مجموع عدد الرقيق خلال الفتره الم...   \n",
       "2  الرياض قامت صباح امس السبت بجوله في سوق السبت ...   \n",
       "3  كان السميلودون اخر اجناس السنوريات سيفيه الاني...   \n",
       "4  وقال سموه في كلمه له بمناسبه اليوم الوطني ان ك...   \n",
       "\n",
       "                                             summary  text_length  \\\n",
       "0  السلوكيات المهدده يمكن ان تكون نتيجه عدم القدر...         1850   \n",
       "1  يقدر عدد الرقيق خلال الفتره الممتده من 650 الي...         1679   \n",
       "2  قامت الرياض صباح امس السبت بجوله في سوق السبت ...          965   \n",
       "3  السميلودون هو اخر اجناس السنوريات سيفيه الانيا...         1549   \n",
       "4  لم يكن اليوم الوطني يعتبر تحقيقا لولا مسيره ال...          847   \n",
       "\n",
       "   text_word_count  text_avg_word_length  text_lexical_diversity  \\\n",
       "0              220              5.663636                0.877273   \n",
       "1              206              5.626214                0.844660   \n",
       "2              120              5.366667                0.783333   \n",
       "3              187              6.363636                0.716578   \n",
       "4              113              5.433628                0.734513   \n",
       "\n",
       "   summary_length  summary_word_count  summary_avg_word_length  \\\n",
       "0             416                  50                 6.020000   \n",
       "1             487                  60                 5.683333   \n",
       "2             281                  36                 5.750000   \n",
       "3             421                  49                 6.795918   \n",
       "4             361                  49                 5.714286   \n",
       "\n",
       "   summary_lexical_diversity  \n",
       "0                   0.920000  \n",
       "1                   0.950000  \n",
       "2                   0.972222  \n",
       "3                   0.918367  \n",
       "4                   0.897959  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113173 14146 14148\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_size = int(len(df) * 0.8)  \n",
    "val_size = int(len(df) * 0.1)    \n",
    "test_size = len(df) - (train_size + val_size)  \n",
    "\n",
    "# Split the dataframe\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size + val_size]\n",
    "test_df = df[train_size + val_size:]\n",
    "\n",
    "print(len(train_df) , len(val_df) ,len(test_df) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "print(\"\\n\\n\")\n",
    "df_with_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save them in CSV files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create directory for Data before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data\"\n",
    "train_df.to_csv(f'{data}/train.csv', index=False)\n",
    "val_df.to_csv(f'{data}/val.csv', index=False)\n",
    "test_df.to_csv(f'{data}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to HF dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for Speed, memory, Compatibility and easy Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df.sample(n=30000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_df = val_df.sample(n=6000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)# convert to HF for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import evaluate\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 4-bit quantization to reduce the model size and memory usage during training or inference, making it faster and more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LORA Config\n",
    "LoRA (Low-Rank Adaptation) is used to fine-tune large models efficiently by updating only a small number of parameters instead of the full model weights, reducing memory and compute requirements.\n",
    "\n",
    "Here, we define a `LoraConfig` to specify how LoRA will be applied, including settings like the rank, dropout, and task type.\n",
    "\n",
    "This configuration will be passed later when setting up the model for training or fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora config\n",
    "lora_config  = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=128,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    # prefix summarize to the input text for AraT5\n",
    "    inputs = [\"summarize: \" + text for text in examples[\"text\"]]\n",
    "    summaries = examples[\"summary\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        summaries,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using DataCollator for Dynamic padding. Rather than a fixed lenght for all Seqs, it would be the longest Seq in the batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=base_model, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics for Model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function calculates various evaluation metrics such as Rouge and Cosine Similarity to assess the performance of the model in generating summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "\n",
    "    # Convert predictions to actual token IDs (in case it's logits or nested)\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    predictions = np.array(predictions, dtype=np.int32)\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "\n",
    "    # clip or mask any invalid values\n",
    "    predictions = np.where(predictions < 0, tokenizer.pad_token_id, predictions)\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "\n",
    "    # replace -100 with pad_token_id for decoding\n",
    "    # labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Print the first 10 values of predictions and labels to inspect them\n",
    "   # print(\"Pred sample:\", predictions[0][:10])  # Check the first 10 token IDs in predictions\n",
    "    #print(\"Label sample:\", labels[0][:10])    \n",
    "\n",
    "    # decoding the predicted\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    preprocessed_preds = [preprocess_text(text) for text in decoded_preds]\n",
    "    preprocessed_labels = [preprocess_text(text) for text in decoded_labels]\n",
    "\n",
    "    # rouge scores \n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=preprocessed_preds,\n",
    "        references=preprocessed_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    # apply to predictions and labels \n",
    "    pred_vectors = tfidf_vectorizer.fit_transform(preprocessed_preds)\n",
    "    label_vectors = tfidf_vectorizer.transform(preprocessed_labels)\n",
    "    \n",
    "    # calc cosine similarity\n",
    "    cosine_similarities = []\n",
    "    for i in range(len(preprocessed_preds)):\n",
    "        # check it the vector is not empty\n",
    "        if pred_vectors[i].nnz > 0 and label_vectors[i].nnz > 0:\n",
    "            similarity = cosine_similarity(pred_vectors[i], label_vectors[i])[0][0]\n",
    "            # Guard against NaN or infinite values\n",
    "            if np.isfinite(similarity):\n",
    "                cosine_similarities.append(similarity)\n",
    "            else:\n",
    "                cosine_similarities.append(0.0)\n",
    "        else:\n",
    "            cosine_similarities.append(0.0)\n",
    "            \n",
    "    avg_cosine_sim = np.mean(cosine_similarities) if cosine_similarities else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"rouge1\": rouge_results[\"rouge1\"],\n",
    "        \"rouge2\": rouge_results[\"rouge2\"],\n",
    "        \"rougeL\": rouge_results[\"rougeL\"],\n",
    "        \"cosine_similarity\": avg_cosine_sim\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a8de15d0e247d2a3dff937fef7de7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072d56816bbd41be8869c2c8f0d33c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14146 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_val = val_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='models',\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,         \n",
    "    gradient_accumulation_steps=1,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.1,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\" ,\n",
    "    evaluation_strategy=\"steps\",  # Evaluate during training\n",
    "    eval_steps=1000,  # Perform evaluation every 1000 steps\n",
    "    save_total_limit=1,  # Save only the best model\n",
    "    metric_for_best_model=\"rouge2\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True  # Generate predictions during evaluation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback , EarlyStoppingCallback\n",
    "\n",
    "\n",
    "class DynamicEvalDatasetCallback(TrainerCallback):\n",
    "    def __init__(self, full_eval_dataset, trainer):\n",
    "        self.full_eval_dataset = full_eval_dataset\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.eval_steps == 0 and state.global_step != 0:\n",
    "            small_val_dataset = self.full_eval_dataset.shuffle(seed=state.global_step).select(range(500))\n",
    "            self.trainer.eval_dataset = small_val_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_27744\\3373706363.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "You are adding a <class 'transformers.integrations.integration_utils.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers.integrations import TensorBoardCallback\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val.select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "       TensorBoardCallback(),\n",
    "        early_stopping_callback                      \n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.add_callback(\n",
    "    DynamicEvalDatasetCallback(full_eval_dataset=tokenized_val, trainer=trainer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16000' max='56588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16000/56588 3:36:44 < 9:09:52, 1.23 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>27.535400</td>\n",
       "      <td>14.279666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>14.252600</td>\n",
       "      <td>8.805470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.679400</td>\n",
       "      <td>2.867957</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010222</td>\n",
       "      <td>0.196288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.844200</td>\n",
       "      <td>2.582166</td>\n",
       "      <td>0.030749</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.030390</td>\n",
       "      <td>0.344386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.568000</td>\n",
       "      <td>2.517195</td>\n",
       "      <td>0.044760</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>0.043712</td>\n",
       "      <td>0.355583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.379000</td>\n",
       "      <td>2.378850</td>\n",
       "      <td>0.051815</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.051682</td>\n",
       "      <td>0.357211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.265300</td>\n",
       "      <td>2.359830</td>\n",
       "      <td>0.037330</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>0.036138</td>\n",
       "      <td>0.382853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.354800</td>\n",
       "      <td>2.332574</td>\n",
       "      <td>0.036278</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.371615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.223200</td>\n",
       "      <td>2.275521</td>\n",
       "      <td>0.034295</td>\n",
       "      <td>0.008649</td>\n",
       "      <td>0.034058</td>\n",
       "      <td>0.379019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.186600</td>\n",
       "      <td>2.295914</td>\n",
       "      <td>0.037586</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>0.037332</td>\n",
       "      <td>0.384246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.201500</td>\n",
       "      <td>2.162124</td>\n",
       "      <td>0.046406</td>\n",
       "      <td>0.009674</td>\n",
       "      <td>0.044625</td>\n",
       "      <td>0.380245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.153900</td>\n",
       "      <td>2.238857</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.008330</td>\n",
       "      <td>0.046213</td>\n",
       "      <td>0.387283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.068700</td>\n",
       "      <td>2.186423</td>\n",
       "      <td>0.049849</td>\n",
       "      <td>0.007467</td>\n",
       "      <td>0.048722</td>\n",
       "      <td>0.371526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.072000</td>\n",
       "      <td>2.187050</td>\n",
       "      <td>0.041329</td>\n",
       "      <td>0.005689</td>\n",
       "      <td>0.040628</td>\n",
       "      <td>0.371309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.135300</td>\n",
       "      <td>2.197747</td>\n",
       "      <td>0.041433</td>\n",
       "      <td>0.003364</td>\n",
       "      <td>0.040135</td>\n",
       "      <td>0.392356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.988000</td>\n",
       "      <td>2.118235</td>\n",
       "      <td>0.038129</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>0.037302</td>\n",
       "      <td>0.377191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16000, training_loss=6.258327457427979, metrics={'train_runtime': 13005.5488, 'train_samples_per_second': 17.404, 'train_steps_per_second': 4.351, 'total_flos': 2.823710932224e+16, 'train_loss': 6.258327457427979, 'epoch': 0.5654909168021489})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output for the first training without eval in training**\n",
    "TrainOutput(global_step=15000, training_loss=3.7461526138305663, metrics={'train_runtime': 4475.4542, 'train_samples_per_second': 13.406, 'train_steps_per_second': 3.352, 'total_flos': 2.588429430864691e+16, 'train_loss': 3.7461526138305663, 'epoch': 2.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=r\"C:\\Users\\ACER\\Downloads\\Arabic text summarization\\Arabic-Text-Summarization\\models\\checkpoint-6000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\ACER\\\\Downloads\\\\Arabic text summarization\\\\Arabic-Text-Summarization\\\\AraT5 Fine tuned\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\ACER\\\\Downloads\\\\Arabic text summarization\\\\Arabic-Text-Summarization\\\\AraT5 Fine tuned\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\ACER\\\\Downloads\\\\Arabic text summarization\\\\Arabic-Text-Summarization\\\\AraT5 Fine tuned\\\\spiece.model',\n",
       " 'C:\\\\Users\\\\ACER\\\\Downloads\\\\Arabic text summarization\\\\Arabic-Text-Summarization\\\\AraT5 Fine tuned\\\\added_tokens.json',\n",
       " 'C:\\\\Users\\\\ACER\\\\Downloads\\\\Arabic text summarization\\\\Arabic-Text-Summarization\\\\AraT5 Fine tuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "trainer.save_model(r\"C:\\Users\\ACER\\Downloads\\Arabic text summarization\\Arabic-Text-Summarization\\AraT5 Fine tuned\")  \n",
    "tokenizer.save_pretrained(r\"C:\\Users\\ACER\\Downloads\\Arabic text summarization\\Arabic-Text-Summarization\\AraT5 Fine tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"do_sample\": False,\n",
    "    \"num_return_sequences\": 3,\n",
    "    \"max_length\": 90,\n",
    "    \"min_length\": 20,\n",
    "    \"num_beams\": 4,\n",
    "    \"repetition_penalty\": 2.5,\n",
    "    \"length_penalty\": 1.5,\n",
    "    \"early_stopping\": True,\n",
    "    #\"temperature\": 0.8,            \n",
    "    #\"top_k\": 50,                   \n",
    "    #\"top_p\": 0.95  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):\n",
    "    input_text = \"تلخيص: \" + preprocess_text(text)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # generate \n",
    "    summary_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        **generation_config\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: \n",
      "في عالمنا المعاصر، أصبحت وسائل التواصل الاجتماعي جزءًا لا يتجزأ من حياتنا اليومية. قد تكون هذه الوسائل سلاحًا ذا حدين؛ فمن جهة، تتيح للأفراد التواصل ومشاركة الأفكار والمعلومات مع الآخرين بسهولة، مما يسهم في تعزيز الوعي العام وتوسيع آفاق المعرفة. ومن جهة أخرى، يمكن أن تكون مصدرًا للمعلومات المضللة والأخبار الزائفة التي تؤثر سلبًا على الرأي العام وتزيد من التوتر الاجتماعي.\n",
      "\n",
      "في الآونة الأخيرة، زاد استخدام منصات مثل فيسبوك وتويتر بشكل كبير، مما أتاح للمستخدمين الفرصة للتفاعل مع الأحداث الجارية على مدار الساعة. إلا أن هذا التفاعل يمكن أن يكون عرضة للتلاعب من قبل أطراف معينة تهدف إلى نشر أفكار متطرفة أو التأثير على الانتخابات العامة. ولذلك، يتوجب على الحكومات والمنظمات الاجتماعية تطوير سياسات فعالة لمكافحة هذه الظواهر.\n",
      "\n",
      "علاوة على ذلك، يجب أن يتحمل المستخدمون مسؤولية كبيرة في اختيار المحتوى الذي يشاركونه والتأكد من صحته قبل نشره. من خلال تعزيز الوعي الرقمي والتثقيف حول كيفية التحقق من المصادر، يمكن تقليل تأثير الأخبار الزائفة والمعلومات المغلوطة.\n",
      "\n",
      "في النهاية، تبقى وسائل التواصل الاجتماعي أداة قوية يمكن استخدامها بشكل إيجابي إذا تمت إدارتها بحذر وعقلانية.\n",
      "\n",
      "Generated Summary: وسائل التواصل الاجتماعي اصبحت جزءا لا يتجزا من حياتنا اليوميه، مما يتيح للافراد التواصل ومشاركه الافكار والمعلومات مع الاخرين بسهوله، مما يسهم في تعزيز الوعي العام وتوسيع افاق المعرفه ويمكن ان تكون مصدرا للمعلومات الزائفه والاخبار الزائفه التي تؤثر سلبا علي الراي العام وتزيد من التوتر الاجتماعي في الاونه الاخيره، زاد استخدام منصات مثل فيسبوك وتويتر بشكل كبير، مما يعزز الوعي العام وتوسيع افاق المعرفه\n"
     ]
    }
   ],
   "source": [
    "# Example Arabic text\n",
    "arabic_text = \"\"\"\n",
    "في عالمنا المعاصر، أصبحت وسائل التواصل الاجتماعي جزءًا لا يتجزأ من حياتنا اليومية. قد تكون هذه الوسائل سلاحًا ذا حدين؛ فمن جهة، تتيح للأفراد التواصل ومشاركة الأفكار والمعلومات مع الآخرين بسهولة، مما يسهم في تعزيز الوعي العام وتوسيع آفاق المعرفة. ومن جهة أخرى، يمكن أن تكون مصدرًا للمعلومات المضللة والأخبار الزائفة التي تؤثر سلبًا على الرأي العام وتزيد من التوتر الاجتماعي.\n",
    "\n",
    "في الآونة الأخيرة، زاد استخدام منصات مثل فيسبوك وتويتر بشكل كبير، مما أتاح للمستخدمين الفرصة للتفاعل مع الأحداث الجارية على مدار الساعة. إلا أن هذا التفاعل يمكن أن يكون عرضة للتلاعب من قبل أطراف معينة تهدف إلى نشر أفكار متطرفة أو التأثير على الانتخابات العامة. ولذلك، يتوجب على الحكومات والمنظمات الاجتماعية تطوير سياسات فعالة لمكافحة هذه الظواهر.\n",
    "\n",
    "علاوة على ذلك، يجب أن يتحمل المستخدمون مسؤولية كبيرة في اختيار المحتوى الذي يشاركونه والتأكد من صحته قبل نشره. من خلال تعزيز الوعي الرقمي والتثقيف حول كيفية التحقق من المصادر، يمكن تقليل تأثير الأخبار الزائفة والمعلومات المغلوطة.\n",
    "\n",
    "في النهاية، تبقى وسائل التواصل الاجتماعي أداة قوية يمكن استخدامها بشكل إيجابي إذا تمت إدارتها بحذر وعقلانية.\n",
    "\"\"\"\n",
    "# Generate summary\n",
    "summary = generate_summary(arabic_text)\n",
    "print(\"Original Text:\", arabic_text)\n",
    "print(\"Generated Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from PyQt5.QtWidgets import (\n",
    "    QApplication, QWidget, QVBoxLayout, QHBoxLayout, QTextEdit, QPushButton, QLabel, QFileDialog\n",
    ")\n",
    "from PyQt5.QtGui import QFont, QTextOption\n",
    "from PyQt5.QtCore import Qt\n",
    "\n",
    "class ArabicTextSummarizer(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Arabic Text Summarizer\")\n",
    "        self.setGeometry(100, 100, 900, 600)\n",
    "        self.setStyleSheet(\"background-color: #2C2C2C; color: #fff;\")\n",
    "        self.init_ui()\n",
    "\n",
    "    def init_ui(self):\n",
    "        font = QFont(\"Comic Sans MS\", 12)\n",
    "        label_font = QFont(\"Comic Sans MS\", 11, QFont.Bold)\n",
    "\n",
    "        layout = QVBoxLayout()\n",
    "\n",
    "        # Input label\n",
    "        input_label = QLabel(\"أدخل نصًا عربيًا أو حمّل ملفًا:\")\n",
    "        input_label.setFont(label_font)\n",
    "        input_label.setStyleSheet(\"color: #3498DB;\")\n",
    "        layout.addWidget(input_label)\n",
    "\n",
    "        # Text input\n",
    "        self.text_input = QTextEdit()\n",
    "        self.text_input.setFont(font)\n",
    "        self.text_input.setStyleSheet(\"background-color: #E0E0E0; color: #2C2C2C;\")\n",
    "        self.text_input.setLayoutDirection(Qt.RightToLeft)\n",
    "        self.text_input.setWordWrapMode(QTextOption.WordWrap)\n",
    "        layout.addWidget(self.text_input)\n",
    "\n",
    "        # Buttons\n",
    "        btn_layout = QHBoxLayout()\n",
    "        browse_btn = QPushButton(\"اختار ملف\")\n",
    "        browse_btn.setFont(font)\n",
    "        browse_btn.setStyleSheet(\"background-color: #3498DB; color: #fff;\")\n",
    "        browse_btn.clicked.connect(self.browse_file)\n",
    "        btn_layout.addWidget(browse_btn)\n",
    "\n",
    "        gen_btn = QPushButton(\"شعبلي الدنيا\")\n",
    "        gen_btn.setFont(font)\n",
    "        gen_btn.setStyleSheet(\"background-color: #3498DB; color: #fff;\")\n",
    "        gen_btn.clicked.connect(self.generate_summary)\n",
    "        btn_layout.addWidget(gen_btn)\n",
    "\n",
    "        layout.addLayout(btn_layout)\n",
    "\n",
    "        # Output label\n",
    "        output_label = QLabel(\"الملخص:\")\n",
    "        output_label.setFont(label_font)\n",
    "        output_label.setStyleSheet(\"color: #3498DB;\")\n",
    "        layout.addWidget(output_label)\n",
    "\n",
    "        # Summary output\n",
    "        self.summary_output = QTextEdit()\n",
    "        self.summary_output.setFont(font)\n",
    "        self.summary_output.setStyleSheet(\"background-color: #E0E0E0; color: #2C2C2C;\")\n",
    "        self.summary_output.setReadOnly(True)\n",
    "        self.summary_output.setLayoutDirection(Qt.RightToLeft)\n",
    "        self.summary_output.setWordWrapMode(QTextOption.WordWrap)\n",
    "        layout.addWidget(self.summary_output)\n",
    "\n",
    "        self.setLayout(layout)\n",
    "\n",
    "    def browse_file(self):\n",
    "        file_path, _ = QFileDialog.getOpenFileName(self, \"Open Text File\", \"\", \"Text Files (*.txt)\")\n",
    "        if file_path:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-16-le') as f:\n",
    "                    content = f.read()\n",
    "                self.text_input.setPlainText(content)\n",
    "            except Exception as e:\n",
    "                self.text_input.setPlainText(f\"خطأ في قراءة الملف: {e}\")\n",
    "\n",
    "    def generate_summary(self):\n",
    "        text = self.text_input.toPlainText().strip()\n",
    "        if not text:\n",
    "            self.summary_output.setPlainText(\"يرجى إدخال نص أو تحميل ملف.\")\n",
    "            return\n",
    "        summary = generate_summary(text)\n",
    "        self.summary_output.setPlainText(summary)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QApplication(sys.argv)\n",
    "    window = ArabicTextSummarizer()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
