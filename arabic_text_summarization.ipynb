{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(76, 111, 175); padding: 10px; border-radius: 10px;\">\n",
    "    <h1 style=\"color: Black ; text-align: center; font-weight: bold; font-family: Comic Sans MS, cursive, sans-serif;\">\n",
    "    Arabic Text Summarization\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model , dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model =\"UBC-NLP/AraT5v2-base-1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'summary'],\n",
       "        num_rows: 141467\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"FahdSeddik/AGS-Corpus\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (141467, 3)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "48a601bb-7a60-40fb-82fc-99e7b41ac008",
       "rows": [
        [
         "118079",
         "هذا ما أكده الفنان أحمد جوهر ل ثقافة اليوم بأنه سيستعين بفنانين من مصر ولبنان للمشاركة في المسلسل التلفزيوني الجديد كوبرا الإرهابي حيث سيشارك بالعمل من مصر الفنان سامي العدل وخليل مرسي ومادلين طبرا من لبنان لكون العمل يحمل أجواء مخابراتية ويجسد التعاون الأمني بين مباحث الكويت والخليج والمخابرات المصرية في مكافحة عصابة تحاول إدخال مواد إرهابية للكويت والخليج، مشيراً أن التصوير سيكون خارج الكويت في أجواء ثلجية في ديسمبر أو يناير المقبل. وأضاف جوهر قائلاً هناك نخبة كبيرة من أبرز الفنانين ونجوم الساحة المحلية ستشارك في هذا العمل بينهم أحمد الصالح، غانم الصالح، حسن البلام، أحمد السلمان، محمد الصيرفي. وقد اختيرت هذه العناصر وغيرها كي يخرج المسلسل بثوب جديد مختلف مع فنانين من العرب لكون قصة وسيناريو المسلسل تتطلب المشاركة بين الجهات الأمنية العربية ضد الإرهاب والتصدي له بكل الوسائل للحفاظ على وحدة وسيادة البلد وممتلكاته ليخرج العمل بشكل جيد. ",
         "قرر الفنان أحمد جوهر استعمال فنانين من مصر ولبنان في مسلسله التلفزيوني الجديد \"كوبرا الإرهابي\"، حيث سيلتحق بالمشروع الفنانون سامي العدل وخليل مرسي من مصر، ومادلين طبرا من لبنان. المسلسل سيتطرق إلى التعاون الأمني بين مباحث الكويت والخليج والمخابرات المصرية في مكافحة عصابة تهريب مواد إرهابية. التصوير سيكون في أجواء ثلجية في ديسمبر أو يناير المقبل، وسيشارك في العمل نخبة من أبرز الفنانين ونجوم الساحة المحلية، بما في ذلك أحمد الصالح وغانم الصالح وحسن البلام وأحمد السلمان ومحمد الصيرفي."
        ],
        [
         "100551",
         "أدانت كوريا الشمالية، الاستخدام المفرط للقوة من قبل الجيش الإسرائيلي مع المتظاهرين الفلسطينيين، ووصفت ما تعرضوا له في مسيرات العودة التي نظمت في الأسابيع الأخيرة بـ\"المجازر الفظيعة\".وبحسب ما نقلت صحيفة \"إكسبريس\" البريطانية، فقد بعث رئيس المجلس الشعبي الأعلى في كوريا الشمالية برقية للرئيس الفلسطيني محمود عباس يعرب فيها عن تضامن بيونغيانغ مع الفلسطينيين.ولقي أكثر من121 شخصا مصرعهم في ربيع العام الحالي كما أصيب أكثر من ألف في مظاهرات على حدود قطاع غزة مع إسرائيل.وذكرت صحيفة \"رودونغ سينمون\" الحكومية في كوريا الشمالية، أن بيونغيانغ أدانت \"الحملة الدموية\" لإسرائيل ضد متظاهري غزة، كمااستنكرت \"المجازر الفظيعة\" و\"العنف العشوائي\" ضد متظاهرين سلميين يطالبون بحقوقهم الشرعية.وأكدت كوريا الشمالية دعمها الثابت لحق الفلسطينيين في إقامة دولة مستقلة عاصمتها في القدس الشرقية.وسبق لكوريا الشمالية أن أعربت عن تضامنها مع القضية الفلسطينية فيمواقف عدة مثل حرب غزة في عام 2014.",
         "أدانت كوريا الشمالية استخدام القوة المفرطة من قبل الجيش الإسرائيلي في مسيرات العودة للفلسطينيين، وأعربت عن تضامنها معهم. لقد أدى ذلك إلى مجازر فظيعة، حيث قتل 121 شخصًا وأصيب أكثر من ألف. وأكدت كوريا الشمالية دعمها لحق الفلسطينيين في إقامة دولة مستقلة بالقدس الشرقية."
        ],
        [
         "115734",
         "وأضاف التقرير، أن الوصول إلى المياه النظيفة وممارسات النظافة الصحية السليمة وغسل اليدين بالصابون، بالإضافة إلى صحة الأطفال وتغذيتهم يقلل من الإصابة بهذا المرض بنسبة تتجاوز . وأوضح التقرير، أن غالبية مراحل إسهال الأطفال تتسم بالاعتدال، ولكن الحالات الحادة منه قد تؤدى إلى فقد كميات كبيرة من السوائل ومنه إلى الجفاف أيضاً. وهذا الجفاف قد ينتهى بالموت إن لم تتم الاستعاضة عن السوائل المفقودة سريعاً. وتقول آن فينمان المديرة التنفيذية لليونيسيف، إن الإسهال يؤدى إلى موت . مليون طفل كل عام، و من الأطفال الذى يصابون بالإسهال فى البلدان النامية تتلقى العلاج الواجب . ووضع التقرير، خطة لإنقاذ حياة الأطفال المصابين بالإسهال، تتكون من سبع نقاط للوقاية والعلاج بداية من الرضاعة الطبيعية وغسل اليدين بالصابون وتحسين الإمدادات المائية من الناحيتين الكمية والنوعية، بالإضافة إلى تعزيز المرافق الصحية العلاج بالزنك، وأخيراً التحصين ضد الحصبة الروتافيروس والاستعاضة عن السوائل المفقودة لمنع الجفاف. ",
         "الوصول إلى المياه النظيفة والنظافة الصحية وغسل اليدين بالصابون يقلل من الإسهال. الإسهال يمكن أن يؤدي إلى فقد سوائل وجفاف قد يؤدي إلى الموت. العلاج والوقاية من الإسهال يشمل الرضاعة الطبيعية، غسل اليدين، تحسين المياه، العلاج بالزنك والتحصين ضد الحصبة والروتافيروس."
        ],
        [
         "72730",
         "واختيرت واحدة من بين أجمل عشر نساء فى العالم خلال حقبة الأربعينيات، وخلال السطور التالية تتحدث الفنانة ندى بسيونى فى تصريحات خاصة لـ اليوم السابع عن خلال كواليس وتصويرها مسلسل هوانم جاردن سيتى بمناسبة ذكرى ميلادها. قالت الفنانة ندى بسيونى إن أبرز عادات الفنانة مديحة يسرى خلال تصويرها للمسلسلات التى شاركت فيها من بينها مسلسل هوانم جاردن سيتى، حرصها على الحضور مبكرا لأماكن التصوير، وتتفقد الديكور قبل التصوير حتى تشعر بأنها فى بيتها لكى تتقن دورها بجدية، وكانت تحضر فنجان القهوة الخاص والذى كان يتميز باللون الكحلى والذهبى . وأضافت ندى، فى تصريحات خاصة لـ اليوم السابع ، أن مديحة يسرى كانت تتقن فن الإتيكيت جيدا، فكانت حريصة كل الحرص فى تمثليها على ترتيب أدق الأشياء، مشيرة إلى أن الفنانة مديحة يسرى كانت تتميز بالهدوء وحديثها الممتع وقدرتها على جذب الآخرين بأناقتها. ",
         "تحدث الفنانة ندى بسيوني عن مديحة يسري وأبرز عاداتها خلال تصويرها مسلسل هوانم جاردن سيتي. كانت مديحة تحضر مبكراً وتتفقد الديكور قبل التصوير، وكانت تتميز بالإتيكيت ورتابة وحديثها الممتع وأناقتها. كما كان لها فنجان قهوة خاص بها."
        ],
        [
         "56588",
         "تعرض خالد للعديد من الانتقادات منذ أسلم، بدءاً من سريته إلى بني جذيمة ليدعوهم إلى الإسلام، والتي لم يبعثه الرسول فيها مقاتلاً، فخرج في ثلاثمائة وخمسين رجلاً، من المهاجرين والأنصار وبني سليم، فلما وصل إليهم وجدهم يحملون السلاح. فسألهم عن الإسلام، فقالوا أنهم أسلموا، وسألهم ما بالهم يحملون السلاح، فقالوا أن بينهم وبين قوم من العرب عداوة، فخافوا أن يكون جند المسلمين هم هؤلاء القوم، فارتاب منهم خالدًا وقد كان بنو جذيمة أهل غارات حتى أنهم عُرفوا «بلعقة الدم»، وكان من قتلاهم في الجاهلية الفاكه بن المغيرة عم خالد، وعوف بن عبد عوف أبو عبد الرحمن بن عوف، فأمرهم بوضع السلاح، فوضعوه. ثم قال لهم استأسروا، فاستأسروا، ثم نادى خالد بضرب أعناقهم، فلبّى بنو سليم، بينما رفض المهاجرون والأنصار الأمر، وشكوه إلى الرسول، بل واتهمه عبد الرحمن بن عوف بأن قتلهم بثأر عمه الفاكه. حين بلغ الرسول ما فعل خالد، غضب غضبًا شديدًا لفعل خالد، ورفع يديه داعيًا إلى الله قائلاً: «اللهم إني أبرأ إليك مما صنع خالد»، وبعث عليا ليودي لهم قتلاهم.ثاني الانتقادات التي وجهت لخالد، كانت يوم قتل مالك بن نويرة في حروب الردة، وإن كانت الروايات قد اختلفت في سبب مقتله، فذكر الطبري في تاريخه أن مقتله كان لخطأ في الفهم ممن تولّوا حراستهم من بني كنانة، وأن مالك أُسر هو ورجال من قومه من بني يربوع في ليلة باردة، فأشفق عليهم خالد، فنادى: «دفئوا أسراكم». وكانت تعني في لغة كنانة القتل، فظنوا أن خالد يعني قتلهم فقتلوهم. بينما ذهب ابن كثير في البداية والنهاية وأبو الفرج الأصفهاني في كتاب الأغاني إلى أن خالدًا دعا إليه مالكًا ليناظره ليرى أهو على دين الإسلام أم أنه ارتد ومنع الزكاة، وفيما هما يتناظران راجع مالك خالدًا، فقال: «ما أخال صاحبكم إلا وقد كان يقول كذا وكذا». قال خالد: «أو ما تعدّه لك صاحبًا؟»، ثم أمر بقتله. وقد روى ابن خلكان أن مالك قال أنه يأتي الصلاة دون الزكاة، وهو ما رفضه خالد قائلاً أنهما معًا لا تقبل واحدة دون أخرى. بل وغالى اليعقوبي في تاريخه أنه قتل مالك ليتزوج من امرأته أم تميم.",
         "خالد تعرض للانتقادات بعد اعتناقه الإسلام. وقد قاد مئات المهاجرين والأنصار وبني سليم في معركة مع قوم عربية حاملين السلاح. خالد سألهم عن إسلامهم وسبب حملهم للسلاح، وأخبروه بوجود عداوة مع القوم العرب. خالد شك فيهم وأمرهم بوضع السلاح. ثم أمر بأسرهم وأعدمهم. بعض المهاجرين والأنصار اعترضوا وشكوا إلى الرسول، وغضب الرسول من فعل خالد ودعا الله للبراءة منه. كما قتل خالد مالك بن نويرة خلال حروب الردة بسبب سوء التفاهم. هذه هي الانتقادات الرئيسية التي واجهها خالد."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118079</th>\n",
       "      <td>هذا ما أكده الفنان أحمد جوهر ل ثقافة اليوم بأن...</td>\n",
       "      <td>قرر الفنان أحمد جوهر استعمال فنانين من مصر ولب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100551</th>\n",
       "      <td>أدانت كوريا الشمالية، الاستخدام المفرط للقوة م...</td>\n",
       "      <td>أدانت كوريا الشمالية استخدام القوة المفرطة من ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115734</th>\n",
       "      <td>وأضاف التقرير، أن الوصول إلى المياه النظيفة وم...</td>\n",
       "      <td>الوصول إلى المياه النظيفة والنظافة الصحية وغسل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72730</th>\n",
       "      <td>واختيرت واحدة من بين أجمل عشر نساء فى العالم خ...</td>\n",
       "      <td>تحدث الفنانة ندى بسيوني عن مديحة يسري وأبرز عا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56588</th>\n",
       "      <td>تعرض خالد للعديد من الانتقادات منذ أسلم، بدءاً...</td>\n",
       "      <td>خالد تعرض للانتقادات بعد اعتناقه الإسلام. وقد ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "118079  هذا ما أكده الفنان أحمد جوهر ل ثقافة اليوم بأن...   \n",
       "100551  أدانت كوريا الشمالية، الاستخدام المفرط للقوة م...   \n",
       "115734  وأضاف التقرير، أن الوصول إلى المياه النظيفة وم...   \n",
       "72730   واختيرت واحدة من بين أجمل عشر نساء فى العالم خ...   \n",
       "56588   تعرض خالد للعديد من الانتقادات منذ أسلم، بدءاً...   \n",
       "\n",
       "                                                  summary  \n",
       "118079  قرر الفنان أحمد جوهر استعمال فنانين من مصر ولب...  \n",
       "100551  أدانت كوريا الشمالية استخدام القوة المفرطة من ...  \n",
       "115734  الوصول إلى المياه النظيفة والنظافة الصحية وغسل...  \n",
       "72730   تحدث الفنانة ندى بسيوني عن مديحة يسري وأبرز عا...  \n",
       "56588   خالد تعرض للانتقادات بعد اعتناقه الإسلام. وقد ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "        'text': dataset['train']['text'],\n",
    "        'summary': dataset['train']['summary']\n",
    "    })\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يمكن تصور السلوكيات المُهَدِدة باعتبارها ثمرة لعدم القدرة على التأقلم مع الدافع الطبيعي التنافسي المتعلق بعلاقات الهيمنة المتبادلة التي تلاحظ عامة بين الحيوانات. بدلاً من ذلك، قد ينتج الترهيب في مجتمع من نوع يكون أفراده اجتماعيين، فالبشر بشكل عام يترددون في الدخول في مواجهة أو تهديد عنيف.وهو سلوك مثله مثل جميع السمات السلوكية يظهر بشكل أزيد أو أقل في كل فرد مع مرور الزمن، ولكنه قد يكون «سلوك تعويضي» ذو أهمية كبيرة بالنسبة للبعض مقارنة بالآخرين. فإن المنظرين السلوكيين كثيراً ما يرون أن السلوكيات المُهَدِدة هي نتيجة لتعرض القائمين بها للتهديد من قبل الآخرين، بما في ذلك الآباء، ورموز السلطة، والرفاق والأشقاء. «استخدام القوة مبرر عندما يعتقد الشخص بشكل منطقي أنها ضرورية للدفاع عن النفس أو الآخرين تجاه الاستخدام الفوري لقوة غير شرعية».و قد يتم استخدام الترهيب بوعي أو بغير وعي، ونسبة من الأشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك نتيجة أفكار مستوعبة بأنانية عن تخصيصه لغرض، أو لفائدة أو للتمكين الذاتي. الترهيب المتصل بالتحامل والتمييز يمكن أن يشمل السلوك «الذي يزعج، يهدد، يرهب، وينذر، أو يضع الشخص في حالة خوف على سلامته... بسبب اعتقاد أو تصور بشأن عرقه أو لونه أو أصله القومي أو نسبه أو جنسه أو دينه أو ممارسته لشعائر دينية أو سنه أو إعاقته أو توجهه الجنسي، بغض النظر عن ما إذا كان ذلك الاعتقاد أو التصور صحيحاً».قد يتجلى الترهيب بطرق مختلفة مثل الاحتكاك البدني، أو تجهم الطلعة، أو التلاعب بالمشاعر، أو الإساءة اللفظية، أو جعل شخصاً يشعر بأنه أقل منك، أو الإحراج المتعمد و/أو الاعتداء الجسدي الصريح. «السلوك قد يشمل، ولكنه ليس محصوراً في، الألقاب، التعليقات والافتراءات المهينة والمقترحات البذيئة والاعتداء والإعاقة وعرقلة أو منع الحركة، واللمس الجارح أو أي تداخل جسدي في الحركة أو العمل العادي، والإهانات المرئية، مثل الملصقات أو الرسوم المهينة».ليس هناك تعريف قانوني في القانون الإنجليزي بشأن ما يشكله سلوك «الترهيب» Intimidation، ولذا فالأمر متروك للمحاكم لاتخاذ قرارها بشأن كل حالة على حدة. ومع ذلك، إذا هدد شخص بالعنف تجاه شخص آخر، فان ذلك قد يشكل جريمة جنائية.\n",
      "1876\n"
     ]
    }
   ],
   "source": [
    "print( df['text'][0] )\n",
    "print(len(df[\"text\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "السلوكيات المهددة يمكن أن تكون نتيجة عدم القدرة على التأقلم مع العلاقات الهيمنة التنافسية المتبادلة بين الحيوانات. قد ينتج الترهيب في مجتمع من النوع الاجتماعي. الترهيب يتم استخدامه بوعي أو بغير وعي، ونسبة الأشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب أفكار أنانية. الترهيب يمكن أن يشمل الاحتكاك البدني، أو التلاعب بالمشاعر، أو الإساءة اللفظية، أو الإحراج المتعمد و/أو الاعتداء الجسدي. قد يشكل تهديد العنف جريمة جنائية.\n",
      "422\n"
     ]
    }
   ],
   "source": [
    "print( df['summary'][0] )\n",
    "print(len(df[\"summary\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ء',\n",
       " 'ءَ',\n",
       " 'آ',\n",
       " 'آب',\n",
       " 'آذار',\n",
       " 'آض',\n",
       " 'آمينَ',\n",
       " 'آناء',\n",
       " 'آنفا',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'آهاً',\n",
       " 'آهٍ',\n",
       " 'آهِ',\n",
       " 'آي',\n",
       " 'أ',\n",
       " 'أبدا',\n",
       " 'أبريل',\n",
       " 'أبو',\n",
       " 'أبٌ',\n",
       " 'أجل',\n",
       " 'أجمع',\n",
       " 'أحد',\n",
       " 'أخبر',\n",
       " 'أخذ',\n",
       " 'أخو',\n",
       " 'أخٌ',\n",
       " 'أربع',\n",
       " 'أربعاء',\n",
       " 'أربعة',\n",
       " 'أربعمئة',\n",
       " 'أربعمائة',\n",
       " 'أرى',\n",
       " 'أسكن',\n",
       " 'أصبح',\n",
       " 'أصلا',\n",
       " 'أضحى',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'أعلم',\n",
       " 'أغسطس',\n",
       " 'أف',\n",
       " 'أفريل',\n",
       " 'أفعل به',\n",
       " 'أفٍّ',\n",
       " 'أقبل',\n",
       " 'أقل',\n",
       " 'أكتوبر',\n",
       " 'أكثر',\n",
       " 'أل',\n",
       " 'ألا',\n",
       " 'ألف',\n",
       " 'ألفى',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أمام',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'أمسى',\n",
       " 'أمّا',\n",
       " 'أن',\n",
       " 'أنا',\n",
       " 'أنبأ',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'أنتِ',\n",
       " 'أنشأ',\n",
       " 'أنى',\n",
       " 'أنًّ',\n",
       " 'أنّى',\n",
       " 'أهلا',\n",
       " 'أو',\n",
       " 'أوت',\n",
       " 'أوشك',\n",
       " 'أول',\n",
       " 'أولئك',\n",
       " 'أولاء',\n",
       " 'أولالك',\n",
       " 'أوه',\n",
       " 'أوّهْ',\n",
       " 'أى',\n",
       " 'أي',\n",
       " 'أيا',\n",
       " 'أيار',\n",
       " 'أيضا',\n",
       " 'أيلول',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'أيها',\n",
       " 'أيّ',\n",
       " 'أيّان',\n",
       " 'أُفٍّ',\n",
       " 'ؤ',\n",
       " 'إحدى',\n",
       " 'إذ',\n",
       " 'إذا',\n",
       " 'إذاً',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'إزاء',\n",
       " 'إلا',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'إليكنّ',\n",
       " 'إليكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إلّا',\n",
       " 'إما',\n",
       " 'إمّا',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'إنَّ',\n",
       " 'إى',\n",
       " 'إي',\n",
       " 'إياك',\n",
       " 'إياكم',\n",
       " 'إياكما',\n",
       " 'إياكن',\n",
       " 'إيانا',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهم',\n",
       " 'إياهما',\n",
       " 'إياهن',\n",
       " 'إياي',\n",
       " 'إيه',\n",
       " 'إيهٍ',\n",
       " 'ئ',\n",
       " 'ا',\n",
       " 'ابتدأ',\n",
       " 'اتخذ',\n",
       " 'اثنا',\n",
       " 'اثنان',\n",
       " 'اثني',\n",
       " 'اثنين',\n",
       " 'اخلولق',\n",
       " 'اربعون',\n",
       " 'اربعين',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'الآن',\n",
       " 'الألاء',\n",
       " 'الألى',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللائي',\n",
       " 'اللاتي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'انبرى',\n",
       " 'انقلب',\n",
       " 'ب',\n",
       " 'بؤسا',\n",
       " 'بئس',\n",
       " 'باء',\n",
       " 'بات',\n",
       " 'بخ',\n",
       " 'بخٍ',\n",
       " 'بس',\n",
       " 'بسّ',\n",
       " 'بضع',\n",
       " 'بطآن',\n",
       " 'بعد',\n",
       " 'بعدا',\n",
       " 'بعض',\n",
       " 'بغتة',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بيد',\n",
       " 'بين',\n",
       " 'بَسْ',\n",
       " 'بَلْهَ',\n",
       " 'ة',\n",
       " 'ت',\n",
       " 'تاء',\n",
       " 'تارة',\n",
       " 'تاسع',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تبدّل',\n",
       " 'تجاه',\n",
       " 'تحت',\n",
       " 'تحوّل',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تسع',\n",
       " 'تسعة',\n",
       " 'تسعمئة',\n",
       " 'تسعمائة',\n",
       " 'تسعون',\n",
       " 'تسعين',\n",
       " 'تشرين',\n",
       " 'تعسا',\n",
       " 'تعلَّم',\n",
       " 'تفعلان',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'تلقاء',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'تموز',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'تَيْنِ',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'ث',\n",
       " 'ثاء',\n",
       " 'ثالث',\n",
       " 'ثامن',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثلاث',\n",
       " 'ثلاثاء',\n",
       " 'ثلاثة',\n",
       " 'ثلاثمئة',\n",
       " 'ثلاثمائة',\n",
       " 'ثلاثون',\n",
       " 'ثلاثين',\n",
       " 'ثم',\n",
       " 'ثمان',\n",
       " 'ثمانمئة',\n",
       " 'ثمانون',\n",
       " 'ثماني',\n",
       " 'ثمانية',\n",
       " 'ثمانين',\n",
       " 'ثمة',\n",
       " 'ثمنمئة',\n",
       " 'ثمَّ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ج',\n",
       " 'جانفي',\n",
       " 'جعل',\n",
       " 'جلل',\n",
       " 'جمعة',\n",
       " 'جميع',\n",
       " 'جنيه',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'جير',\n",
       " 'جيم',\n",
       " 'ح',\n",
       " 'حاء',\n",
       " 'حادي',\n",
       " 'حار',\n",
       " 'حاشا',\n",
       " 'حاي',\n",
       " 'حبذا',\n",
       " 'حبيب',\n",
       " 'حتى',\n",
       " 'حجا',\n",
       " 'حدَث',\n",
       " 'حرى',\n",
       " 'حزيران',\n",
       " 'حسب',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'حمو',\n",
       " 'حمٌ',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'حيَّ',\n",
       " 'حَذارِ',\n",
       " 'خ',\n",
       " 'خاء',\n",
       " 'خاصة',\n",
       " 'خال',\n",
       " 'خامس',\n",
       " 'خبَّر',\n",
       " 'خلا',\n",
       " 'خلافا',\n",
       " 'خلف',\n",
       " 'خمس',\n",
       " 'خمسة',\n",
       " 'خمسمئة',\n",
       " 'خمسمائة',\n",
       " 'خمسون',\n",
       " 'خمسين',\n",
       " 'خميس',\n",
       " 'د',\n",
       " 'دال',\n",
       " 'درهم',\n",
       " 'درى',\n",
       " 'دواليك',\n",
       " 'دولار',\n",
       " 'دون',\n",
       " 'دونك',\n",
       " 'ديسمبر',\n",
       " 'دينار',\n",
       " 'ذ',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذال',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذانِ',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذهب',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذيت',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ذَيْنِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ر',\n",
       " 'رأى',\n",
       " 'راء',\n",
       " 'رابع',\n",
       " 'راح',\n",
       " 'رجع',\n",
       " 'رزق',\n",
       " 'رويدك',\n",
       " 'ريال',\n",
       " 'ريث',\n",
       " 'رُبَّ',\n",
       " 'ز',\n",
       " 'زاي',\n",
       " 'زعم',\n",
       " 'زود',\n",
       " 'س',\n",
       " 'ساء',\n",
       " 'سابع',\n",
       " 'سادس',\n",
       " 'سبت',\n",
       " 'سبتمبر',\n",
       " 'سبحان',\n",
       " 'سبع',\n",
       " 'سبعة',\n",
       " 'سبعمئة',\n",
       " 'سبعمائة',\n",
       " 'سبعون',\n",
       " 'سبعين',\n",
       " 'ست',\n",
       " 'ستة',\n",
       " 'ستمئة',\n",
       " 'ستمائة',\n",
       " 'ستون',\n",
       " 'ستين',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سرعان',\n",
       " 'سقى',\n",
       " 'سمعا',\n",
       " 'سنتيم',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'سين',\n",
       " 'ش',\n",
       " 'شباط',\n",
       " 'شبه',\n",
       " 'شتان',\n",
       " 'شتانَ',\n",
       " 'شرع',\n",
       " 'شمال',\n",
       " 'شيكل',\n",
       " 'شين',\n",
       " 'شَتَّانَ',\n",
       " 'ص',\n",
       " 'صاد',\n",
       " 'صار',\n",
       " 'صباح',\n",
       " 'صبر',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'صهٍ',\n",
       " 'صهْ',\n",
       " 'ض',\n",
       " 'ضاد',\n",
       " 'ضحوة',\n",
       " 'ط',\n",
       " 'طاء',\n",
       " 'طاق',\n",
       " 'طالما',\n",
       " 'طرا',\n",
       " 'طفق',\n",
       " 'طَق',\n",
       " 'ظ',\n",
       " 'ظاء',\n",
       " 'ظلّ',\n",
       " 'ظنَّ',\n",
       " 'ع',\n",
       " 'عاد',\n",
       " 'عاشر',\n",
       " 'عامة',\n",
       " 'عجبا',\n",
       " 'عدا',\n",
       " 'عدَّ',\n",
       " 'عسى',\n",
       " 'عشر',\n",
       " 'عشرة',\n",
       " 'عشرون',\n",
       " 'عشرين',\n",
       " 'عل',\n",
       " 'علق',\n",
       " 'علم',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'علًّ',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'عوض',\n",
       " 'عيانا',\n",
       " 'عين',\n",
       " 'عَدَسْ',\n",
       " 'غ',\n",
       " 'غادر',\n",
       " 'غالبا',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'غير',\n",
       " 'غين',\n",
       " 'ف',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فاء',\n",
       " 'فبراير',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'فلا',\n",
       " 'فلان',\n",
       " 'فلس',\n",
       " 'فمن',\n",
       " 'فو',\n",
       " 'فوق',\n",
       " 'في',\n",
       " 'فيفري',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'ق',\n",
       " 'قاطبة',\n",
       " 'قاف',\n",
       " 'قام',\n",
       " 'قبل',\n",
       " 'قد',\n",
       " 'قرش',\n",
       " 'قطّ',\n",
       " 'قلما',\n",
       " 'ك',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأنّ',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'كاد',\n",
       " 'كاف',\n",
       " 'كان',\n",
       " 'كانون',\n",
       " 'كثيرا',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كرب',\n",
       " 'كسا',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كلَّا',\n",
       " 'كلّما',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كن',\n",
       " 'كى',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'كِخ',\n",
       " 'ل',\n",
       " 'لئن',\n",
       " 'لا',\n",
       " 'لا سيما',\n",
       " 'لات',\n",
       " 'لاسيما',\n",
       " 'لام',\n",
       " 'لبيك',\n",
       " 'لدن',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لعلَّ',\n",
       " 'لعمر',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكنَّ',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لمّا',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'ليت',\n",
       " 'ليرة',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'م',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ما',\n",
       " 'ما أفعله',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مائة',\n",
       " 'مادام',\n",
       " 'ماذا',\n",
       " 'مارس',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ماي',\n",
       " 'مايو',\n",
       " 'متى',\n",
       " 'مثل',\n",
       " 'مذ',\n",
       " 'مرّة',\n",
       " 'مساء',\n",
       " 'مع',\n",
       " 'معاذ',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'مكانَك',\n",
       " 'مليم',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منذ',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'ميم',\n",
       " 'ن',\n",
       " 'نا',\n",
       " 'نبَّا',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'نفس',\n",
       " 'نوفمبر',\n",
       " 'نون',\n",
       " 'نيسان',\n",
       " 'نيف',\n",
       " 'نَخْ',\n",
       " 'نَّ',\n",
       " 'ه',\n",
       " 'هؤلاء',\n",
       " 'ها',\n",
       " 'هاء',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاكَ',\n",
       " 'هاهنا',\n",
       " 'هبّ',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هللة',\n",
       " 'هلم',\n",
       " 'هلّا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'همزة',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'هيّا',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَجْ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذَيْنِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَيْهات',\n",
       " 'و',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'وا',\n",
       " 'واحد',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'واهاً',\n",
       " 'واو',\n",
       " 'وجد',\n",
       " 'وراءَك',\n",
       " 'ورد',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهب',\n",
       " 'وهو',\n",
       " 'وَيْ',\n",
       " 'وُشْكَانَ',\n",
       " 'ى',\n",
       " 'ي',\n",
       " 'يا',\n",
       " 'ياء',\n",
       " 'يفعلان',\n",
       " 'يفعلون',\n",
       " 'يمين',\n",
       " 'ين',\n",
       " 'يناير',\n",
       " 'يوان',\n",
       " 'يورو',\n",
       " 'يوليو',\n",
       " 'يونيو',\n",
       " 'ّأيّان'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_stopwords = set(stopwords.words('arabic')) \n",
    "arabic_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_patterns = {\n",
    "            # Remove diacritics\n",
    "            r'[\\u064B-\\u065F\\u0670]': '',\n",
    "            # Normalize Arabic characters\n",
    "            r'[إأآا]': 'ا',\n",
    "            r'ى': 'ي',\n",
    "            r'ة': 'ه'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "        for pattern, replacement in arabic_patterns.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the words \n",
    "the Arabic stop words we load earlier and the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stopwords(stopwords_set):\n",
    "    cleaned_stopwords = set()\n",
    "    for word in stopwords_set:\n",
    "        # normalize them \n",
    "        word = normalize_arabic(word)\n",
    "        cleaned_stopwords.add(word)\n",
    "    return cleaned_stopwords\n",
    "\n",
    "# cleaned set \n",
    "arabic_stopwords = clean_stopwords(arabic_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        # remove whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # remove  urls\n",
    "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "        # remove emails\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "        arabic_punctuation = '،؛؟'\n",
    "        # remove other punctuation \n",
    "        punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        \n",
    "        punctuation_to_remove = ''.join(ch for ch in punctuation if ch not in arabic_punctuation)\n",
    "        return text.translate(str.maketrans('', '', punctuation_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ara_tokenizer(text):\n",
    "        text = normalize_arabic(text)\n",
    "        tokens = re.findall(r'\\w+', text)\n",
    "        return [token for token in tokens if token not in arabic_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will extract the basic features for any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\n",
    "            'length': 0,\n",
    "            'word_count': 0,\n",
    "            'avg_word_length': 0,\n",
    "            'lexical_diversity': 0\n",
    "        }\n",
    "    \n",
    "    text = normalize_arabic(text)  \n",
    "    tokens = ara_tokenizer(text)\n",
    "\n",
    "    features = {\n",
    "        'length': len(text),\n",
    "        'word_count': len(tokens),\n",
    "        'avg_word_length': sum(len(word) for word in tokens) / max(1, len(tokens)),\n",
    "        'lexical_diversity': len(set(tokens)) / max(1, len(tokens))\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining all the function in one to implement preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "        # check if its a text \n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "            \n",
    "        text = str(text).strip()\n",
    "        \n",
    "        text = clean_text(text)\n",
    "        text = remove_punctuation(text)\n",
    "        text = normalize_arabic(text)\n",
    "            \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define TF_IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just an extra step to implement cosain similarty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=500, \n",
    "            ngram_range=(1, 2),\n",
    "            tokenizer= lambda text: ara_tokenizer(text),\n",
    "            stop_words=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'يمكن تصور السلوكيات المهدده باعتبارها ثمره لعدم القدره علي التاقلم مع الدافع الطبيعي التنافسي المتعلق بعلاقات الهيمنه المتبادله التي تلاحظ عامه بين الحيوانات بدلا من ذلك، قد ينتج الترهيب في مجتمع من نوع يكون افراده اجتماعيين، فالبشر بشكل عام يترددون في الدخول في مواجهه او تهديد عنيفوهو سلوك مثله مثل جميع السمات السلوكيه يظهر بشكل ازيد او اقل في كل فرد مع مرور الزمن، ولكنه قد يكون «سلوك تعويضي» ذو اهميه كبيره بالنسبه للبعض مقارنه بالاخرين فان المنظرين السلوكيين كثيرا ما يرون ان السلوكيات المهدده هي نتيجه لتعرض القائمين بها للتهديد من قبل الاخرين، بما في ذلك الاباء، ورموز السلطه، والرفاق والاشقاء «استخدام القوه مبرر عندما يعتقد الشخص بشكل منطقي انها ضروريه للدفاع عن النفس او الاخرين تجاه الاستخدام الفوري لقوه غير شرعيه»و قد يتم استخدام الترهيب بوعي او بغير وعي، ونسبه من الاشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك نتيجه افكار مستوعبه بانانيه عن تخصيصه لغرض، او لفائده او للتمكين الذاتي الترهيب المتصل بالتحامل والتمييز يمكن ان يشمل السلوك «الذي يزعج، يهدد، يرهب، وينذر، او يضع الشخص في حاله خوف علي سلامته بسبب اعتقاد او تصور بشان عرقه او لونه او اصله القومي او نسبه او جنسه او دينه او ممارسته لشعائر دينيه او سنه او اعاقته او توجهه الجنسي، بغض النظر عن ما اذا كان ذلك الاعتقاد او التصور صحيحا»قد يتجلي الترهيب بطرق مختلفه مثل الاحتكاك البدني، او تجهم الطلعه، او التلاعب بالمشاعر، او الاساءه اللفظيه، او جعل شخصا يشعر بانه اقل منك، او الاحراج المتعمد واو الاعتداء الجسدي الصريح «السلوك قد يشمل، ولكنه ليس محصورا في، الالقاب، التعليقات والافتراءات المهينه والمقترحات البذيئه والاعتداء والاعاقه وعرقله او منع الحركه، واللمس الجارح او اي تداخل جسدي في الحركه او العمل العادي، والاهانات المرئيه، مثل الملصقات او الرسوم المهينه»ليس هناك تعريف قانوني في القانون الانجليزي بشان ما يشكله سلوك «الترهيب» Intimidation، ولذا فالامر متروك للمحاكم لاتخاذ قرارها بشان كل حاله علي حده ومع ذلك، اذا هدد شخص بالعنف تجاه شخص اخر، فان ذلك قد يشكل جريمه جنائيه'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing to df_with_features_with_features text and summary \n",
    "df['text'] = df['text'].apply(preprocess_text) \n",
    "df['summary'] = df['summary'].apply(preprocess_text)  \n",
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'السلوكيات المهدده يمكن ان تكون نتيجه عدم القدره علي التاقلم مع العلاقات الهيمنه التنافسيه المتبادله بين الحيوانات قد ينتج الترهيب في مجتمع من النوع الاجتماعي الترهيب يتم استخدامه بوعي او بغير وعي، ونسبه الاشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب افكار انانيه الترهيب يمكن ان يشمل الاحتكاك البدني، او التلاعب بالمشاعر، او الاساءه اللفظيه، او الاحراج المتعمد واو الاعتداء الجسدي قد يشكل تهديد العنف جريمه جنائيه'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison\n",
    "before=\"السلوكيات المهددة يمكن أن تكون نتيجة عدم القدرة على التأقلم مع العلاقات الهيمنة التنافسية المتبادلة بين الحيوانات. قد ينتج الترهيب في مجتمع من النوع الاجتماعي. الترهيب يتم استخدامه بوعي أو بغير وعي، ونسبة الأشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب أفكار أنانية. الترهيب يمكن أن يشمل الاحتكاك البدني، أو التلاعب بالمشاعر، أو الإساءة اللفظية، أو الإحراج المتعمد و/أو الاعتداء الجسدي. قد يشكل تهديد العنف جريمة جنائية\"\n",
    "after =\"السلوكيات المهدده يمكن ان تكون نتيجه عدم القدره علي التاقلم مع العلاقات الهيمنه التنافسيه المتبادله بين الحيوانات قد ينتج الترهيب في مجتمع من النوع الاجتماعي الترهيب يتم استخدامه بوعي او بغير وعي، ونسبه الاشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب افكار انانيه الترهيب يمكن ان يشمل الاحتكاك البدني، او التلاعب بالمشاعر، او الاساءه اللفظيه، او الاحراج المتعمد واو الاعتداء الجسدي قد يشكل تهديد العنف جريمه جنائيه\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features\n",
    "text_features = pd.DataFrame(df['text'].apply(extract_features).tolist())\n",
    "text_features.columns = [f'text_{col}' for col in text_features.columns]\n",
    "\n",
    "summary_features = pd.DataFrame(df['summary'].apply(extract_features).tolist())\n",
    "summary_features.columns = [f'summary_{col}' for col in summary_features.columns]\n",
    "\n",
    "# combine the data with the extracted features by column\n",
    "df_with_features = pd.concat([df, text_features, summary_features], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_avg_word_length",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_lexical_diversity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "summary_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "summary_avg_word_length",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "summary_lexical_diversity",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "dce06ec5-5085-4336-8270-a82cb9848948",
       "rows": [
        [
         "0",
         "يمكن تصور السلوكيات المهدده باعتبارها ثمره لعدم القدره علي التاقلم مع الدافع الطبيعي التنافسي المتعلق بعلاقات الهيمنه المتبادله التي تلاحظ عامه بين الحيوانات بدلا من ذلك، قد ينتج الترهيب في مجتمع من نوع يكون افراده اجتماعيين، فالبشر بشكل عام يترددون في الدخول في مواجهه او تهديد عنيفوهو سلوك مثله مثل جميع السمات السلوكيه يظهر بشكل ازيد او اقل في كل فرد مع مرور الزمن، ولكنه قد يكون «سلوك تعويضي» ذو اهميه كبيره بالنسبه للبعض مقارنه بالاخرين فان المنظرين السلوكيين كثيرا ما يرون ان السلوكيات المهدده هي نتيجه لتعرض القائمين بها للتهديد من قبل الاخرين، بما في ذلك الاباء، ورموز السلطه، والرفاق والاشقاء «استخدام القوه مبرر عندما يعتقد الشخص بشكل منطقي انها ضروريه للدفاع عن النفس او الاخرين تجاه الاستخدام الفوري لقوه غير شرعيه»و قد يتم استخدام الترهيب بوعي او بغير وعي، ونسبه من الاشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك نتيجه افكار مستوعبه بانانيه عن تخصيصه لغرض، او لفائده او للتمكين الذاتي الترهيب المتصل بالتحامل والتمييز يمكن ان يشمل السلوك «الذي يزعج، يهدد، يرهب، وينذر، او يضع الشخص في حاله خوف علي سلامته بسبب اعتقاد او تصور بشان عرقه او لونه او اصله القومي او نسبه او جنسه او دينه او ممارسته لشعائر دينيه او سنه او اعاقته او توجهه الجنسي، بغض النظر عن ما اذا كان ذلك الاعتقاد او التصور صحيحا»قد يتجلي الترهيب بطرق مختلفه مثل الاحتكاك البدني، او تجهم الطلعه، او التلاعب بالمشاعر، او الاساءه اللفظيه، او جعل شخصا يشعر بانه اقل منك، او الاحراج المتعمد واو الاعتداء الجسدي الصريح «السلوك قد يشمل، ولكنه ليس محصورا في، الالقاب، التعليقات والافتراءات المهينه والمقترحات البذيئه والاعتداء والاعاقه وعرقله او منع الحركه، واللمس الجارح او اي تداخل جسدي في الحركه او العمل العادي، والاهانات المرئيه، مثل الملصقات او الرسوم المهينه»ليس هناك تعريف قانوني في القانون الانجليزي بشان ما يشكله سلوك «الترهيب» Intimidation، ولذا فالامر متروك للمحاكم لاتخاذ قرارها بشان كل حاله علي حده ومع ذلك، اذا هدد شخص بالعنف تجاه شخص اخر، فان ذلك قد يشكل جريمه جنائيه",
         "السلوكيات المهدده يمكن ان تكون نتيجه عدم القدره علي التاقلم مع العلاقات الهيمنه التنافسيه المتبادله بين الحيوانات قد ينتج الترهيب في مجتمع من النوع الاجتماعي الترهيب يتم استخدامه بوعي او بغير وعي، ونسبه الاشخاص الذين يستخدمونه بوعي ربما يفعلون ذلك بسبب افكار انانيه الترهيب يمكن ان يشمل الاحتكاك البدني، او التلاعب بالمشاعر، او الاساءه اللفظيه، او الاحراج المتعمد واو الاعتداء الجسدي قد يشكل تهديد العنف جريمه جنائيه",
         "1850",
         "220",
         "5.663636363636364",
         "0.8772727272727273",
         "416",
         "50",
         "6.02",
         "0.92"
        ],
        [
         "1",
         "يقدر المؤرخون مجموع عدد الرقيق خلال الفتره الممتده بدءا من مطلع التاريخ الاسلامي في عام 650 وحتي الغاء الاتجار بالرقيق في شبه الجزيره العربيه في منتصف القرن العشرين بما يتراوح عدده من 10 الي 18 مليون شخص كان يطلق عليهم اسم الزنج كان تجار الرقيق من شرق افريقيا يستجلبون هؤلاء العبيد وينقلوهم الي شبه الجزيره العربيه وغيرها من المناطق المجاوره للبيع في اسواق النخاسه كانت اعداد الرقيق الذين استجلبوا الي المنطقه من خلال هذه الممارسه اكثر بكثير من العبيد المنقولين الي الامريكيتين اثرت عده عوامل علي عدم بروز اسلاف هذا المكون السكاني في المجتمعات العربيه المعاصره خلال القرن الحادي والعشرين اولها ان معظم هؤلاء العبيد كانوا جوار من النساء حيث كان الطلب علي شرائهن مرتفعا في شبه الجزيره العربيه والمناطق المجاوره حيث خدمن كمحظيات اما العبيد من الرجال فكان يتم اخصائهم حتي يخدموا كحرس يلبون حريم اسيادهم هذا وقد كان معدل وفيات الرقيق السود الذين اشتغلوا بالسخره عاليا الي حد ملحوظ كان الاطفال الذين ولدوا من تزاوج الساده العرب مع الجواري من ذوي اصول عرقيه مختلطه وكان هؤلاء يدمجون مع عائلات ابائهم ويتمتعون بالحريه علي عكس امهاتهم، وذلك نظرا لطبيعه المجتمع الذي كان افراده ينسبون حصرا الي ابائهم ولذلك لم يبقي من مجتمعات العرب الافارقه المنحدره من اسلاف هؤلاء الرقيق في شبه الجزيره العربيه والدول المجاوره سوي القليليوجد في عدد من البلدان الشرق اوسطيه عده اقليات عربيه من ذوي اصول افريقيه مثل العراق التي فيها نحو 12 مليون عراقي اسود اشتكت هذه المجتمعات من تعرضها لتاريخ حافل من التمييز والعنصريه ومع ذلك فقد بذل الافروعراقيون المنحدرون من طبقه «الزنج» جهودا حثيثه من اجل نيل اعتراف حكومي بوضعهم كاقليه مما سيمكنهم من الحصول علي مقاعد تمثيليه لهم في البرلمان كغيرهم من فئات الشعب العراقي تعتبر معظم هذه المجتمعات المنتشره في المنطقه انفسها كعرب وافارقه وفقا للباحث علمين مزروي وزملائه",
         "يقدر عدد الرقيق خلال الفتره الممتده من 650 الي منتصف القرن العشرين بين 10 الي 18 مليون شخص كان تجار الرقيق يستجلبون العبيد من شرق افريقيا وينقلونهم الي شبه الجزيره العربيه وغيرها للبيع في اسواق النخاسه لم يبق من مجتمعات العرب الافارقه المنحدره من اسلاف هؤلاء الرقيق سوي القليل يوجد في عدد من البلدان الشرق اوسطيه اقليات عربيه من ذوي اصول افريقيه، مثل العراق، حيث يشتكي هؤلاء المجتمعات من التمييز والعنصريه الافروعراقيون يسعون لنيل اعتراف حكومي كاقليه للحصول علي مقاعد تمثيليه في البرلمان",
         "1679",
         "206",
         "5.62621359223301",
         "0.8446601941747572",
         "487",
         "60",
         "5.683333333333334",
         "0.95"
        ],
        [
         "2",
         "الرياض قامت صباح امس السبت بجوله في سوق السبت بالحليله احد الاسواق الشعبيه ورصدت التذمر الكبير جراء الارتفاع المخيف في الاسعار، فقد اشار يوسف احد الباعه ان مرحله الباذنجان ارتفعت يوم امس فقط عن السعر قبل يومين فقد ارتفعت من الي ريالا، واكد يوسف ان الحال ذاته ينطبق علي الجزر والكوسه والباميا والبطاطس والبصل والخس فقد تضاعفت الي الضعف قبل دخول رمضان فيما تضاعفت اكثر من الضعفين عن سعرها في رمضان السنه الماضيه، فيما اورد البائع راضي مثالا علي الخيار فقد كان يباع الكرتون منه في بدايه رمضان السنه الماضيه بخمسه ريالات فيما يباع اليوم امس بعشرين ريالا، فيما كانت تباع الباميا بعشره ريالات والان تباع بثلاثين ريالا كما يباع صندوق الطماطم المستورد من تركيا ب ريالا فيما كان بسبعه ريالات بعض الموردين برر في حديث له مع الرياض سبب الارتفاع الي الحرب في لبنان مذكرا ان لبنان احد المصدرين الرئيسيين للخضار والفاكهه للمملكه، كما عزوا الارتفاع الي ارتفاع حراره الجو هذا العام مبينين ان رمضان في العام الماضي كان الطقس فيه افضل بكثير من هذا العام مما اثر في ضعف الانتاج المحلي",
         "قامت الرياض صباح امس السبت بجوله في سوق السبت بالحليله ولاحظت ارتفاعا كبيرا في اسعار الباذنجان، الجزر، الكوسه، الباميا، البطاطس، البصل، والخس وتضاعفت الاسعار قبل رمضان باكثر من الضعفين مقارنه بالعام الماضي وبرر بعض الموردين الارتفاع بسبب الحرب في لبنان وارتفاع حراره الجو هذا العام",
         "965",
         "120",
         "5.366666666666666",
         "0.7833333333333333",
         "281",
         "36",
         "5.75",
         "0.9722222222222222"
        ],
        [
         "3",
         "كان السميلودون اخر اجناس السنوريات سيفيه الانياب التي عاشت علي وجه الارض، وهو ايضا اشهرها، ويبلغ من مقدار شهرته ان عامه الناس تعتقد بان تسميتي «السنور سيفي الانياب» و«السميلودون» مترادفتان وتعنيان شيئا واحدا، علي الرغم من ان الاولي اسم عام يستخدم في وصف عده اجناس سنوريات بائده تمتعت بانياب علويه فائقه الطول ويلاحظ ان معظم تلك الاجناس تطورت ونشات تقاربيا نتيجه تشابه ظروف معيشتها، دون ان تكون وثيقه الصله ببعضها بالضروره ويلاحظ ايضا ان حيوانات اخري من غير السنوريات ظهرت لديها تلك الميزه كونها عاشت حياه شبيهه بحياه السنوريات سيفيه الانياب من ابرز تلك الحيوانات غورغونيات الوجه الاسم العلمي Gorgonopsia والثايلاكوسميليدات الاسم العلمي Thylacosmilidae وخنجريات الاسنان الاسم العلمي Machaeroides والنمرڤيديات الاسم العلمي Nimravidae والباربروفيلسات الاسم العلمي Barbourofelidae وسيفيات الاسنان الاسم العلمي Machairodontinae تشتمل فصيله السنوريات الحقيقيه علي سيفيات الاسنان بصفتها فصيله ضمنها، فهي تصنيف فرعي ادني منها، والاخيره تقسم بدورها الي ثلاث قبائل سيفيات الاسنان المزيفه الاسم العلمي Metailurini، والسنوريات حرابيه الاسنان الاسم العلمي Homotherini، والسنوريات طعانه الاسنان الاسم العلمي Smilodontini، والقبيله الاخيره هي ما ينتمي اليها جنس السميلودون تتميز السنوريات المنتميه لقبيله طعانه الاسنان بانيابها الطويله النحيله المسننه، او عديمه التسنين في بعض الحالات، بالمقابل تتميز السنوريات حرابيه الاسنان بانياب اقصر واعرض واشد تفلطحا واخشن تسنينا اما السنوريات المنتميه الي قبيله سيفيات الاسنان المزيفه، فكانت اقل تخصصا، وانيابها اقصر واقل تفلطحا، حتي ان بعض الباحثين يخرجونها من نطاق فصيله سيفيات الاسنان لافتقادها لما يميز اعضاء هذه الفصيله",
         "السميلودون هو اخر اجناس السنوريات سيفيه الانياب، وهو اشهرها تسميتي السنور سيفي الانياب والسميلودون ليست مترادفتين تطورت العديد من الاجناس التي تمتعت بانياب علويه فائقه الطول بشكل متقارب تشمل الحيوانات الاخري التي ظهرت لديها تلك الميزه غورغونيات الوجه، الثايلاكوسميليدات، خنجريات الاسنان، النمرفيديات، الباربروفيلسات، وسيفيات الاسنان السميلودون ينتمي الي قبيله سنوريات طعانه الاسنان، ويتميز بانيابه الطويله النحيله المسننه",
         "1549",
         "187",
         "6.363636363636363",
         "0.7165775401069518",
         "421",
         "49",
         "6.795918367346939",
         "0.9183673469387755"
        ],
        [
         "4",
         "وقال سموه في كلمه له بمناسبه اليوم الوطني ان كل هذا لم يكن ليتحقق لولا توفيق الله ثم مواصله ابناء الملك عبدالعزيز رحمه الله لمسيره التنميه والبناء وتطوير هذه البلاد الغاليه والنهوض بها في جميع المجالات حتي وصلنا الي هذا العهد الزاهر بقياده خادم الحرمين الشريفين الملك عبدالله بن عبدالعزيز وسمو ولي العهد الامين وسمو النائب الثاني حفظهم الله واصبحنا نري الانجازات تتحقق في بلادنا علي مختلف الاصعده الاقتصاديه والسياسيه والاجتماعيه ونحن في هذا اليوم نعيش نهضه نماء وتطور بعد مرور عاما علي توحيد بلادنا الغاليه ونحمد الله علي ما انعم به علينا من نعم كثيره بفضله تعالي ثم بدعم ومتابعه ولاه الامر يحفظهم الله وسال سموه الله تعالي ان يديم علي وطننا نعمه الامن والاستقرار في ظل حكومه خادم الحرمين الشريفين الملك عبدالله بن عبدالعزيز وسمو ولي العهد الامين وسمو النائب الثاني حفظهم الله داعيا المولي العلي القدير ان يوفق قيادتنا لما فيه خير امتنا الاسلاميه",
         "لم يكن اليوم الوطني يعتبر تحقيقا لولا مسيره التنميه والبناء والتطوير التي قادها الملك عبدالعزيز رحمه الله واستمرار ابنائه بقياده الملك عبدالله بن عبدالعزيز وسمو ولي العهد الامين وسمو النائب الثاني، شهدت البلاد تقدما اقتصاديا وسياسيا واجتماعيا نحن نشهد نهضه في جميع المجالات بفضل دعم ومتابعه حكومتنا نطلب من الله الامن والاستقرار لوطننا تحت قياده حكومتنا الرشيده",
         "847",
         "113",
         "5.433628318584071",
         "0.7345132743362832",
         "361",
         "49",
         "5.714285714285714",
         "0.8979591836734694"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_avg_word_length</th>\n",
       "      <th>text_lexical_diversity</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>summary_word_count</th>\n",
       "      <th>summary_avg_word_length</th>\n",
       "      <th>summary_lexical_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>يمكن تصور السلوكيات المهدده باعتبارها ثمره لعد...</td>\n",
       "      <td>السلوكيات المهدده يمكن ان تكون نتيجه عدم القدر...</td>\n",
       "      <td>1850</td>\n",
       "      <td>220</td>\n",
       "      <td>5.663636</td>\n",
       "      <td>0.877273</td>\n",
       "      <td>416</td>\n",
       "      <td>50</td>\n",
       "      <td>6.020000</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>يقدر المؤرخون مجموع عدد الرقيق خلال الفتره الم...</td>\n",
       "      <td>يقدر عدد الرقيق خلال الفتره الممتده من 650 الي...</td>\n",
       "      <td>1679</td>\n",
       "      <td>206</td>\n",
       "      <td>5.626214</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>487</td>\n",
       "      <td>60</td>\n",
       "      <td>5.683333</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الرياض قامت صباح امس السبت بجوله في سوق السبت ...</td>\n",
       "      <td>قامت الرياض صباح امس السبت بجوله في سوق السبت ...</td>\n",
       "      <td>965</td>\n",
       "      <td>120</td>\n",
       "      <td>5.366667</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>281</td>\n",
       "      <td>36</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>كان السميلودون اخر اجناس السنوريات سيفيه الاني...</td>\n",
       "      <td>السميلودون هو اخر اجناس السنوريات سيفيه الانيا...</td>\n",
       "      <td>1549</td>\n",
       "      <td>187</td>\n",
       "      <td>6.363636</td>\n",
       "      <td>0.716578</td>\n",
       "      <td>421</td>\n",
       "      <td>49</td>\n",
       "      <td>6.795918</td>\n",
       "      <td>0.918367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>وقال سموه في كلمه له بمناسبه اليوم الوطني ان ك...</td>\n",
       "      <td>لم يكن اليوم الوطني يعتبر تحقيقا لولا مسيره ال...</td>\n",
       "      <td>847</td>\n",
       "      <td>113</td>\n",
       "      <td>5.433628</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>361</td>\n",
       "      <td>49</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>0.897959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  يمكن تصور السلوكيات المهدده باعتبارها ثمره لعد...   \n",
       "1  يقدر المؤرخون مجموع عدد الرقيق خلال الفتره الم...   \n",
       "2  الرياض قامت صباح امس السبت بجوله في سوق السبت ...   \n",
       "3  كان السميلودون اخر اجناس السنوريات سيفيه الاني...   \n",
       "4  وقال سموه في كلمه له بمناسبه اليوم الوطني ان ك...   \n",
       "\n",
       "                                             summary  text_length  \\\n",
       "0  السلوكيات المهدده يمكن ان تكون نتيجه عدم القدر...         1850   \n",
       "1  يقدر عدد الرقيق خلال الفتره الممتده من 650 الي...         1679   \n",
       "2  قامت الرياض صباح امس السبت بجوله في سوق السبت ...          965   \n",
       "3  السميلودون هو اخر اجناس السنوريات سيفيه الانيا...         1549   \n",
       "4  لم يكن اليوم الوطني يعتبر تحقيقا لولا مسيره ال...          847   \n",
       "\n",
       "   text_word_count  text_avg_word_length  text_lexical_diversity  \\\n",
       "0              220              5.663636                0.877273   \n",
       "1              206              5.626214                0.844660   \n",
       "2              120              5.366667                0.783333   \n",
       "3              187              6.363636                0.716578   \n",
       "4              113              5.433628                0.734513   \n",
       "\n",
       "   summary_length  summary_word_count  summary_avg_word_length  \\\n",
       "0             416                  50                 6.020000   \n",
       "1             487                  60                 5.683333   \n",
       "2             281                  36                 5.750000   \n",
       "3             421                  49                 6.795918   \n",
       "4             361                  49                 5.714286   \n",
       "\n",
       "   summary_lexical_diversity  \n",
       "0                   0.920000  \n",
       "1                   0.950000  \n",
       "2                   0.972222  \n",
       "3                   0.918367  \n",
       "4                   0.897959  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113173 14146 14148\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_size = int(len(df) * 0.8)  \n",
    "val_size = int(len(df) * 0.1)    \n",
    "test_size = len(df) - (train_size + val_size)  \n",
    "\n",
    "# Split the dataframe\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size + val_size]\n",
    "test_df = df[train_size + val_size:]\n",
    "\n",
    "print(len(train_df) , len(val_df) ,len(test_df) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "print(\"\\n\\n\")\n",
    "df_with_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save them in CSV files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create directory for Data before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data\"\n",
    "train_df.to_csv(f'{data}/train.csv', index=False)\n",
    "val_df.to_csv(f'{data}/val.csv', index=False)\n",
    "test_df.to_csv(f'{data}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to HF dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for Speed, memory, Compatibility and easy Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/val.csv\")\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)# convert to HF for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "train_df = train_df.sample(n=30000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"data/val.csv\")\n",
    "val_df = val_df.sample(n=6000, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)# convert to HF for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import evaluate\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 4-bit quantization to reduce the model size and memory usage during training or inference, making it faster and more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LORA Config\n",
    "LoRA (Low-Rank Adaptation) is used to fine-tune large models efficiently by updating only a small number of parameters instead of the full model weights, reducing memory and compute requirements.\n",
    "\n",
    "Here, we define a `LoraConfig` to specify how LoRA will be applied, including settings like the rank, dropout, and task type.\n",
    "\n",
    "This configuration will be passed later when setting up the model for training or fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora config\n",
    "lora_config  = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    # prefix summarize to the input text for AraT5\n",
    "    inputs = [\"summarize: \" + text for text in examples[\"text\"]]\n",
    "    summaries = examples[\"summary\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        summaries,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using DataCollator for Dynamic padding. Rather than a fixed lenght for all Seqs, it would be the longest Seq in the batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=base_model, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics for Model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function calculates various evaluation metrics such as Rouge and Cosine Similarity to assess the performance of the model in generating summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # replace -100 with pad_token_id for decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # decoding the predicted\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    preprocessed_preds = [preprocess_text(text) for text in decoded_preds]\n",
    "    preprocessed_labels = [preprocess_text(text) for text in decoded_labels]\n",
    "\n",
    "    # rouge scores \n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=preprocessed_preds,\n",
    "        references=preprocessed_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    # TF-IDF Vectorization for cosine similarity\n",
    "    eval_vectorizer = TfidfVectorizer(\n",
    "        max_features=500,\n",
    "        ngram_range=(1, 2),\n",
    "        tokenizer=lambda text: ara_tokenizer(text) if text else [],\n",
    "        stop_words=None\n",
    "    )\n",
    "    \n",
    "    # apply to predictions and labels \n",
    "    pred_vectors = eval_vectorizer.fit_transform(preprocessed_preds)\n",
    "    label_vectors = eval_vectorizer.transform(preprocessed_labels)\n",
    "    \n",
    "    # calc cosine similarity\n",
    "    cosine_similarities = []\n",
    "    for i in range(len(preprocessed_preds)):\n",
    "        # check it the vector is not empty\n",
    "        if pred_vectors[i].nnz > 0 and label_vectors[i].nnz > 0:\n",
    "            similarity = cosine_similarity(pred_vectors[i], label_vectors[i])[0][0]\n",
    "            # Guard against NaN or infinite values\n",
    "            if np.isfinite(similarity):\n",
    "                cosine_similarities.append(similarity)\n",
    "            else:\n",
    "                cosine_similarities.append(0.0)\n",
    "        else:\n",
    "            cosine_similarities.append(0.0)\n",
    "            \n",
    "    avg_cosine_sim = np.mean(cosine_similarities) if cosine_similarities else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"rouge1\": rouge_results[\"rouge1\"],\n",
    "        \"rouge2\": rouge_results[\"rouge2\"],\n",
    "        \"rougeL\": rouge_results[\"rougeL\"],\n",
    "        \"cosine_similarity\": avg_cosine_sim\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478b529e74824151b1cdc0076ae39e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f571f97d481f4965a50fb102d23f617a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='models',\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,         \n",
    "    gradient_accumulation_steps=1,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\" ,\n",
    "    evaluation_strategy=\"steps\",  # Evaluate during training\n",
    "    eval_steps=500,  # Perform evaluation every 500 steps\n",
    "    save_total_limit=1,  # Save only the best model\n",
    "    metric_for_best_model=\"rouge2\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True  # Generate predictions during evaluation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback , EarlyStoppingCallback\n",
    "\n",
    "\n",
    "class DynamicEvalDatasetCallback(TrainerCallback):\n",
    "    def __init__(self, full_eval_dataset, trainer):\n",
    "        self.full_eval_dataset = full_eval_dataset\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.eval_steps == 0 and state.global_step != 0:\n",
    "            small_val_dataset = self.full_eval_dataset.shuffle(seed=state.global_step).select(range(500))\n",
    "            self.trainer.eval_dataset = small_val_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_6584\\652832434.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val.select(range(500)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        dynamic_eval_callback,  \n",
    "        early_stopping_callback                      \n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_eval_callback = DynamicEvalDatasetCallback(full_eval_dataset=tokenized_val, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update training arguments to include evaluation during training\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='models',\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_steps=0,\n",
    "    logging_steps=25,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    evaluation_strategy=\"steps\",  # Evaluate during training\n",
    "    eval_steps=100,  # Perform evaluation every 100 steps\n",
    "    save_total_limit=1,  # Save only the best model\n",
    "    metric_for_best_model=\"rouge2\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,  # Generate predictions during evaluation\n",
    ")\n",
    "\n",
    "# Initialize the trainer with updated arguments\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/15000 5:44:49 < 1:08:58, 0.60 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.227500</td>\n",
       "      <td>2.260849</td>\n",
       "      <td>0.039032</td>\n",
       "      <td>0.008131</td>\n",
       "      <td>0.038570</td>\n",
       "      <td>0.349898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.130000</td>\n",
       "      <td>2.259577</td>\n",
       "      <td>0.037384</td>\n",
       "      <td>0.008972</td>\n",
       "      <td>0.037228</td>\n",
       "      <td>0.346257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.299000</td>\n",
       "      <td>2.246920</td>\n",
       "      <td>0.038490</td>\n",
       "      <td>0.008052</td>\n",
       "      <td>0.037771</td>\n",
       "      <td>0.349770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.162500</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>0.039233</td>\n",
       "      <td>0.345411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.149200</td>\n",
       "      <td>2.228877</td>\n",
       "      <td>0.040465</td>\n",
       "      <td>0.008553</td>\n",
       "      <td>0.040716</td>\n",
       "      <td>0.349918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.159100</td>\n",
       "      <td>2.232502</td>\n",
       "      <td>0.041172</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>0.040085</td>\n",
       "      <td>0.346453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.129100</td>\n",
       "      <td>2.220701</td>\n",
       "      <td>0.041236</td>\n",
       "      <td>0.008644</td>\n",
       "      <td>0.040762</td>\n",
       "      <td>0.346785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.147700</td>\n",
       "      <td>2.211735</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.010299</td>\n",
       "      <td>0.039001</td>\n",
       "      <td>0.348830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.173100</td>\n",
       "      <td>2.207544</td>\n",
       "      <td>0.039571</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>0.038650</td>\n",
       "      <td>0.356687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.181000</td>\n",
       "      <td>2.201705</td>\n",
       "      <td>0.039691</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.038277</td>\n",
       "      <td>0.348396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.173500</td>\n",
       "      <td>2.201690</td>\n",
       "      <td>0.036212</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>0.035290</td>\n",
       "      <td>0.346703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.186600</td>\n",
       "      <td>2.200461</td>\n",
       "      <td>0.038485</td>\n",
       "      <td>0.009628</td>\n",
       "      <td>0.036956</td>\n",
       "      <td>0.357212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.152600</td>\n",
       "      <td>2.197217</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>0.035846</td>\n",
       "      <td>0.353907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.243500</td>\n",
       "      <td>2.193897</td>\n",
       "      <td>0.036686</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>0.035499</td>\n",
       "      <td>0.348096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.199700</td>\n",
       "      <td>2.191972</td>\n",
       "      <td>0.036589</td>\n",
       "      <td>0.009949</td>\n",
       "      <td>0.034644</td>\n",
       "      <td>0.353891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.187400</td>\n",
       "      <td>2.191679</td>\n",
       "      <td>0.040479</td>\n",
       "      <td>0.009705</td>\n",
       "      <td>0.039084</td>\n",
       "      <td>0.353957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.237000</td>\n",
       "      <td>2.188754</td>\n",
       "      <td>0.038396</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>0.036970</td>\n",
       "      <td>0.356529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.161300</td>\n",
       "      <td>2.189055</td>\n",
       "      <td>0.038761</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>0.036658</td>\n",
       "      <td>0.347022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.087200</td>\n",
       "      <td>2.191688</td>\n",
       "      <td>0.038244</td>\n",
       "      <td>0.009371</td>\n",
       "      <td>0.037330</td>\n",
       "      <td>0.358405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>2.190822</td>\n",
       "      <td>0.039016</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>0.036778</td>\n",
       "      <td>0.361693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.165200</td>\n",
       "      <td>2.187489</td>\n",
       "      <td>0.038888</td>\n",
       "      <td>0.009972</td>\n",
       "      <td>0.036996</td>\n",
       "      <td>0.349944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.140500</td>\n",
       "      <td>2.187874</td>\n",
       "      <td>0.039027</td>\n",
       "      <td>0.009687</td>\n",
       "      <td>0.036710</td>\n",
       "      <td>0.347551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.135400</td>\n",
       "      <td>2.187519</td>\n",
       "      <td>0.038605</td>\n",
       "      <td>0.010005</td>\n",
       "      <td>0.036839</td>\n",
       "      <td>0.358048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.088500</td>\n",
       "      <td>2.187860</td>\n",
       "      <td>0.038273</td>\n",
       "      <td>0.009880</td>\n",
       "      <td>0.036681</td>\n",
       "      <td>0.350364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.120900</td>\n",
       "      <td>2.187975</td>\n",
       "      <td>0.038795</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.036905</td>\n",
       "      <td>0.361977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\utils\\other.py:1107: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 20034c79-bc3b-4da5-91bf-9ca6ac5e4d6e)') - silently ignoring the lookup for the file config.json in UBC-NLP/AraT5v2-base-1024.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in UBC-NLP/AraT5v2-base-1024 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\utils\\other.py:1107: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 28bae642-660b-430e-97fb-1f6aeceea45b)') - silently ignoring the lookup for the file config.json in UBC-NLP/AraT5v2-base-1024.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in UBC-NLP/AraT5v2-base-1024 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\utils\\other.py:1107: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0890a2e5-af7f-47b3-a750-c773339477d2)') - silently ignoring the lookup for the file config.json in UBC-NLP/AraT5v2-base-1024.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in UBC-NLP/AraT5v2-base-1024 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\utils\\other.py:1107: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d764359e-e059-44b7-8ff8-d3d0eb26fdfe)') - silently ignoring the lookup for the file config.json in UBC-NLP/AraT5v2-base-1024.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in UBC-NLP/AraT5v2-base-1024 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\utils\\other.py:1107: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b3ef09cc-f832-4f4a-b656-3d50fe2fc0cd)') - silently ignoring the lookup for the file config.json in UBC-NLP/AraT5v2-base-1024.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in UBC-NLP/AraT5v2-base-1024 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=3.1733781103515626, metrics={'train_runtime': 20690.0166, 'train_samples_per_second': 2.9, 'train_steps_per_second': 0.725, 'total_flos': 2.158002803902464e+16, 'train_loss': 3.1733781103515626, 'epoch': 1.6666666666666665})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output for the first training without eval in training**\n",
    "TrainOutput(global_step=15000, training_loss=3.7461526138305663, metrics={'train_runtime': 4475.4542, 'train_samples_per_second': 13.406, 'train_steps_per_second': 3.352, 'total_flos': 2.588429430864691e+16, 'train_loss': 3.7461526138305663, 'epoch': 2.0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluation_results)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\trainer_seq2seq.py:197\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\trainer.py:4098\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4095\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[0;32m   4096\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m-> 4098\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[0;32m   4100\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\trainer.py:1120\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[1;34m(self, eval_dataset)\u001b[0m\n\u001b[0;32m   1111\u001b[0m dataloader_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size,\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollate_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_collator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersistent_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_persistent_workers,\n\u001b[0;32m   1117\u001b[0m }\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterableDataset):\n\u001b[1;32m-> 1120\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_eval_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1121\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_drop_last\n\u001b[0;32m   1122\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefetch_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_prefetch_factor\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._get_eval_sampler\u001b[1;34m(self, eval_dataset)\u001b[0m\n\u001b[0;32m   1060\u001b[0m         lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m     model_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLengthGroupedSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SequentialSampler(eval_dataset)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\trainer_pt_utils.py:645\u001b[0m, in \u001b[0;36mLengthGroupedSampler.__init__\u001b[1;34m(self, batch_size, dataset, lengths, model_input_name, generator)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], BatchEncoding))\n\u001b[0;32m    639\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m model_input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    640\u001b[0m     ):\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    644\u001b[0m         )\n\u001b[1;32m--> 645\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(feature[model_input_name]) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lengths, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    647\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to List[int]...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\transformers\\trainer_pt_utils.py:645\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], BatchEncoding))\n\u001b[0;32m    639\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m model_input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    640\u001b[0m     ):\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    644\u001b[0m         )\n\u001b[1;32m--> 645\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(feature[model_input_name]) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lengths, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    647\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to List[int]...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\datasets\\arrow_dataset.py:2384\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2382\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2383\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mARROW_READER_BATCH_SIZE_IN_DATASET_ITER\n\u001b[1;32m-> 2384\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pa_subtable \u001b[38;5;129;01min\u001b[39;00m table_iter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[0;32m   2386\u001b[0m         pa_subtable_ex \u001b[38;5;241m=\u001b[39m pa_subtable\u001b[38;5;241m.\u001b[39mslice(i, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\llama\\lib\\site-packages\\datasets\\table.py:2395\u001b[0m, in \u001b[0;36mtable_iter\u001b[1;34m(table, batch_size, drop_last_batch)\u001b[0m\n\u001b[0;32m   2393\u001b[0m chunks_buffer_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   2394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mto_reader(max_chunksize\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[1;32m-> 2395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2396\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   2397\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m chunks_buffer_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m<\u001b[39m batch_size:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate()\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_length\": 128,\n",
    "    \"min_length\": 30,\n",
    "    \"num_beams\": 5,\n",
    "    \"length_penalty\": 1.5,\n",
    "    \"early_stopping\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):\n",
    "    input_text = \"summarize: \" + preprocess_text(text)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # generate \n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        **generation_config\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
